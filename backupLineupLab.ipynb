{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LineupLab**: NBA Matchup Prediction using Transformer Networks\n",
    "\n",
    "## Project Overview\n",
    "This project is part of the final requirement for the **Introduction to Deep Learning** course. The objective is to develop a machine learning model that predicts NBA matchup outcomes based on player lineups and team configurations. \n",
    "\n",
    "By leveraging the BallDontLie API, we will retrieve, clean, and process NBA data to create a dataset suitable for training and testing. A transformer-based deep learning model will be implemented using PyTorch to analyze player lineups and generate predictions.\n",
    "\n",
    "## Goals\n",
    "1. **Data Exploration**: Analyze and preprocess NBA data to ensure compatibility with the model.\n",
    "2. **Model Creation**: Build a transformer network to learn relationships between players in a lineup and predict game outcomes.\n",
    "3. **Hyperparameter Tuning**: Experiment with learning rate, optimizer, number of epochs, and other hyperparameters to optimize performance.\n",
    "4. **Evaluation and Analysis**: Evaluate model performance using metrics such as accuracy, loss, and F1-score. Provide insights into the model's strengths, limitations, and potential improvements.\n",
    "\n",
    "## Key Features\n",
    "- **Transformer Networks**: Leveraging multi-head attention to capture player and team relationships.\n",
    "- **Comprehensive Dataset**: Utilizing player stats, game results, and team information from the BallDontLie API.\n",
    "- **Visualization and Analysis**: Incorporating visual representations of data distributions, training progress, and performance metrics.\n",
    "\n",
    "This notebook will serve as the main documentation for the project, including all steps from data retrieval to model evaluation.\n",
    "\n",
    "Created with the aid of ChatGPT 4o - Using AI to build AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import requests  # For API requests\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np  # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For data visualization\n",
    "import torch  # For deep learning model implementation\n",
    "import torch.nn as nn  # For neural network components\n",
    "import torch.optim as optim  # For optimization algorithms\n",
    "from torch.utils.data import Dataset, DataLoader  # For data handling in PyTorch\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "from nba_api.stats.endpoints import leaguedashlineups\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Ensure plots are displayed inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Display confirmation message\n",
    "print(\"Libraries successfully loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **I. Data Exploration and Preparation**\n",
    "\n",
    "### Overview\n",
    "In this section, we will collect, prepare, and process NBA data using the **BallDontLie API** to build a dataset for training a transformer-based deep learning model.\n",
    "\n",
    "### Goals and Actions\n",
    "1. **Data Collection**:\n",
    "   - Retrieve detailed NBA game data (2003â€“2023) using game IDs.\n",
    "   - Collect individual player statistics (e.g., minutes played, offensive and defensive ratings, usage percentages) for each game.\n",
    "   - Extract team information (e.g., home and away team IDs, scores) and player metadata (e.g., names, positions).\n",
    "\n",
    "2. **Data Cleaning**:\n",
    "   - Normalize data formats, particularly for time strings (e.g., parsing minutes played).\n",
    "   - Handle missing or incomplete data by assigning default or null values where necessary.\n",
    "   - Rename and standardize column names (e.g., `id` â†’ `game_id`) for consistency.\n",
    "\n",
    "3. **Parallelized Processing**:\n",
    "   - Implement a **threaded processing solution** to speed up API calls and data extraction.\n",
    "   - Use incremental saving to ensure progress is retained during long data processing tasks.\n",
    "\n",
    "4. **Dataset Preparation**:\n",
    "   - Combine game-level and player-level statistics into a single dataset.\n",
    "   - Structure the data for model input, including creating columns for the top 12 players (home and away) with their associated metrics.\n",
    "   - Include player positions directly from the API to enhance the feature set for modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching game data for 2003-2023, Home/Away Teams, Date, & Score\n",
    "\n",
    "# Base URL and API Key\n",
    "BASE_URL = \"https://api.balldontlie.io/v1/games\"\n",
    "API_KEY = \"3c5f3508-5962-4809-8f3e-2b42449e253f\"\n",
    "\n",
    "# Headers for the API request\n",
    "HEADERS = {\n",
    "    \"Authorization\": API_KEY\n",
    "}\n",
    "\n",
    "# Define the seasons to retrieve (2003 to 2023)\n",
    "START_YEAR = 2003\n",
    "END_YEAR = 2023\n",
    "\n",
    "# List to store game data\n",
    "all_games = []\n",
    "\n",
    "# Function to fetch games for a specific season\n",
    "def fetch_games_for_season(season):\n",
    "    cursor = None  # Start without a cursor\n",
    "    while True:\n",
    "        print(f\"Fetching season {season}, cursor: {cursor}\")\n",
    "        \n",
    "        # Construct the API URL with cursor for pagination\n",
    "        url = f\"{BASE_URL}?seasons[]={season}&per_page=100\"\n",
    "        if cursor:\n",
    "            url += f\"&cursor={cursor}\"\n",
    "        \n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error fetching data: {response.status_code}. Retrying in 60 seconds...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "\n",
    "        data = response.json()\n",
    "        \n",
    "        # Ensure the response contains new data\n",
    "        if not data['data']:\n",
    "            print(f\"No more data found for season {season}, exiting loop.\")\n",
    "            break  # Exit loop if no more games are found\n",
    "\n",
    "        # Add new games to the list\n",
    "        all_games.extend(data['data'])\n",
    "        print(f\"Fetched {len(data['data'])} games. Total games collected: {len(all_games)}\")\n",
    "        \n",
    "        # Update the cursor for the next page\n",
    "        cursor = data.get('meta', {}).get('next_cursor', None)\n",
    "        if not cursor:  # No more pages\n",
    "            print(f\"All pages fetched for season {season}.\")\n",
    "            break\n",
    "        \n",
    "        # Throttle requests to avoid hitting rate limits\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Fetch data for each season\n",
    "for season in range(START_YEAR, END_YEAR + 1):\n",
    "    fetch_games_for_season(season)\n",
    "\n",
    "# Process the collected data into a DataFrame\n",
    "print(\"Processing data into DataFrame...\")\n",
    "games_data = [\n",
    "    {\n",
    "        \"id\": game[\"id\"],\n",
    "        \"date\": game[\"date\"],\n",
    "        \"season\": game[\"season\"],\n",
    "        \"status\": game[\"status\"],\n",
    "        \"home_team_score\": game[\"home_team_score\"],\n",
    "        \"visitor_team_score\": game[\"visitor_team_score\"],\n",
    "        \"home_team_name\": game[\"home_team\"][\"full_name\"],\n",
    "        \"home_team_id\": game[\"home_team\"][\"id\"],\n",
    "        \"visitor_team_name\": game[\"visitor_team\"][\"full_name\"],\n",
    "        \"visitor_team_id\": game[\"visitor_team\"][\"id\"]\n",
    "    }\n",
    "    for game in all_games\n",
    "]\n",
    "\n",
    "games_df = pd.DataFrame(games_data)\n",
    "\n",
    "# Save the data to a CSV file\n",
    "output_file = \"games_2003_2023.csv\"\n",
    "games_df.to_csv(output_file, index=False)\n",
    "print(f\"Data saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Success!** We have successfully gathered the scores for every game from 2003-2023\n",
    "\n",
    "#### Now we will enrich this data-frame with the top 12 most used players for each team including their Player-ID's, Position, Minutes, Offensive Rating, Defensive Rating, and Usage Pctg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 12 Home Players:\n",
      "           full_name           team_name  minutes_played position  \\\n",
      "15      Derek Fisher  Los Angeles Lakers       37.300000            \n",
      "16       Gary Payton  Los Angeles Lakers       36.000000            \n",
      "12     Devean George  Los Angeles Lakers       34.583333            \n",
      "14  Shaquille O'Neal  Los Angeles Lakers       32.000000            \n",
      "13       Karl Malone  Los Angeles Lakers       29.000000            \n",
      "17     Bryon Russell  Los Angeles Lakers       20.783333            \n",
      "18      Horace Grant  Los Angeles Lakers       20.000000            \n",
      "21       Kareem Rush  Los Angeles Lakers       14.000000            \n",
      "20     Jannero Pargo  Los Angeles Lakers        9.000000            \n",
      "19       Luke Walton  Los Angeles Lakers        7.000000            \n",
      "22       Kobe Bryant  Los Angeles Lakers        0.000000            \n",
      "23  Slava Medvedenko  Los Angeles Lakers        0.000000            \n",
      "\n",
      "    offensive_rating  defensive_rating  usage_percentage  \n",
      "15             113.0              84.8             0.200  \n",
      "16             121.3              85.5             0.256  \n",
      "12             112.5              94.4             0.114  \n",
      "14             113.8              87.7             0.282  \n",
      "13             124.6              84.4             0.197  \n",
      "17              97.7              97.8             0.196  \n",
      "18              97.7             100.0             0.063  \n",
      "21              93.1             113.8             0.194  \n",
      "20              68.4             116.7             0.250  \n",
      "19              45.5              76.9             0.231  \n",
      "22               0.0               0.0             0.000  \n",
      "23               0.0               0.0             0.000  \n",
      "\n",
      "Top 12 Away Players:\n",
      "          full_name         team_name  minutes_played position  \\\n",
      "1     Dirk Nowitzki  Dallas Mavericks       33.183333        F   \n",
      "0    Antoine Walker  Dallas Mavericks       33.000000        F   \n",
      "5    Antawn Jamison  Dallas Mavericks       32.850000            \n",
      "3    Michael Finley  Dallas Mavericks       28.983333            \n",
      "4        Steve Nash  Dallas Mavericks       28.000000            \n",
      "7         Tony Delk  Dallas Mavericks       27.450000            \n",
      "6       Travis Best  Dallas Mavericks       16.000000            \n",
      "2     Danny Fortson  Dallas Mavericks       14.000000            \n",
      "8    Eduardo Najera  Dallas Mavericks       13.000000        F   \n",
      "9       Josh Howard  Dallas Mavericks        5.000000            \n",
      "10    Shawn Bradley  Dallas Mavericks        4.000000            \n",
      "11  Marquis Daniels  Dallas Mavericks        4.000000            \n",
      "\n",
      "    offensive_rating  defensive_rating  usage_percentage  \n",
      "1               82.9             114.7             0.333  \n",
      "0               88.7             116.2             0.188  \n",
      "5              106.9             107.1             0.185  \n",
      "3               91.8             112.9             0.197  \n",
      "4               82.0             123.3             0.206  \n",
      "7              101.8             103.6             0.152  \n",
      "6              108.3              88.6             0.158  \n",
      "2               44.8             124.1             0.032  \n",
      "8              107.7             100.0             0.103  \n",
      "9               90.0              90.0             0.417  \n",
      "10              87.5              71.4             0.200  \n",
      "11             100.0              71.4             0.111  \n",
      "Data saved to miniexp_games_2003_2023_top12_with_positions.csv.\n"
     ]
    }
   ],
   "source": [
    "#Example of how we are going to expand data with information from stats and advanced_stats - we will do this on a loop for every game we have gathered\n",
    "game_id = 15486  # Example game ID\n",
    "\n",
    "# Base URLs and API Key\n",
    "BASE_URL_STATS = \"https://api.balldontlie.io/v1/stats\"\n",
    "BASE_URL_ADVANCED = \"https://api.balldontlie.io/v1/stats/advanced\"\n",
    "API_KEY = \"3c5f3508-5962-4809-8f3e-2b42449e253f\"\n",
    "HEADERS = {\"Authorization\": API_KEY}\n",
    "\n",
    "# Function to fetch stats\n",
    "def fetch_stats(url, game_id):\n",
    "    response = requests.get(f\"{url}?game_ids[]={game_id}&per_page=100\", headers=HEADERS)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"data\"]\n",
    "    else:\n",
    "        raise Exception(f\"Error fetching stats: {response.status_code}, {response.text}\")\n",
    "\n",
    "# Refined parse_minutes function\n",
    "def parse_minutes(value):\n",
    "    try:\n",
    "        if isinstance(value, str):\n",
    "            if \":\" in value:  # Time string in \"MM:SS\" format\n",
    "                parts = value.split(\":\")\n",
    "                minutes = int(parts[0])\n",
    "                seconds = int(parts[1])\n",
    "                return minutes + seconds / 60  # Convert seconds to fractional minutes\n",
    "            elif value.isdigit():  # Whole number string like \"38\"\n",
    "                return float(value)  # Convert directly to float\n",
    "        return 0  # Default for invalid or missing values\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing minutes value '{value}': {e}\")\n",
    "        return 0\n",
    "\n",
    "# Fetch data\n",
    "base_stats = fetch_stats(BASE_URL_STATS, game_id)\n",
    "advanced_stats = fetch_stats(BASE_URL_ADVANCED, game_id)\n",
    "\n",
    "# Convert to DataFrames\n",
    "base_df = pd.DataFrame(base_stats)\n",
    "adv_df = pd.DataFrame(advanced_stats)\n",
    "\n",
    "# Parse minutes played\n",
    "base_df[\"minutes_played\"] = base_df[\"min\"].apply(parse_minutes)\n",
    "\n",
    "# Merge base and advanced stats on player ID\n",
    "base_df[\"player_id\"] = base_df[\"player\"].apply(lambda x: x[\"id\"])\n",
    "adv_df[\"player_id\"] = adv_df[\"player\"].apply(lambda x: x[\"id\"])\n",
    "adv_df[\"position\"] = adv_df[\"player\"].apply(lambda x: x.get(\"position\", None))  # Extract position\n",
    "merged_df = pd.merge(\n",
    "    base_df,\n",
    "    adv_df[[\"player_id\", \"offensive_rating\", \"defensive_rating\", \"usage_percentage\", \"position\"]],\n",
    "    on=\"player_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Add team and player full name\n",
    "merged_df[\"team_id\"] = merged_df[\"team\"].apply(lambda x: x[\"id\"])\n",
    "merged_df[\"team_name\"] = merged_df[\"team\"].apply(lambda x: x[\"full_name\"])\n",
    "merged_df[\"full_name\"] = merged_df[\"player\"].apply(lambda x: f\"{x['first_name']} {x['last_name']}\")\n",
    "\n",
    "# Split into home and away teams and take top 12 players by minutes played\n",
    "home_team_id = 14  # Los Angeles Lakers\n",
    "away_team_id = 7   # Dallas Mavericks\n",
    "home_players = merged_df[merged_df[\"team_id\"] == home_team_id].nlargest(12, \"minutes_played\")\n",
    "away_players = merged_df[merged_df[\"team_id\"] == away_team_id].nlargest(12, \"minutes_played\")\n",
    "\n",
    "# Print top 12 players for each team\n",
    "print(\"\\nTop 12 Home Players:\")\n",
    "print(home_players[[\"full_name\", \"team_name\", \"minutes_played\", \"position\", \"offensive_rating\", \"defensive_rating\", \"usage_percentage\"]])\n",
    "\n",
    "print(\"\\nTop 12 Away Players:\")\n",
    "print(away_players[[\"full_name\", \"team_name\", \"minutes_played\", \"position\", \"offensive_rating\", \"defensive_rating\", \"usage_percentage\"]])\n",
    "\n",
    "# Combine results into a single row for testing\n",
    "game_row = {\n",
    "    \"id\": game_id,\n",
    "    \"date\": \"2003-10-28\",\n",
    "    \"season\": 2003,\n",
    "    \"status\": \"Final\",\n",
    "    \"home_team_score\": 109,\n",
    "    \"visitor_team_score\": 93,\n",
    "    \"home_team_name\": \"Los Angeles Lakers\",\n",
    "    \"home_team_id\": home_team_id,\n",
    "    \"visitor_team_name\": \"Dallas Mavericks\",\n",
    "    \"visitor_team_id\": away_team_id,\n",
    "}\n",
    "\n",
    "for i in range(1, 13):\n",
    "    if i <= len(home_players):\n",
    "        game_row[f\"home_player_{i}_id\"] = home_players.iloc[i - 1][\"player_id\"]\n",
    "        game_row[f\"home_player_{i}_name\"] = home_players.iloc[i - 1][\"full_name\"]\n",
    "        game_row[f\"home_player_{i}_minutes\"] = home_players.iloc[i - 1][\"minutes_played\"]\n",
    "        game_row[f\"home_player_{i}_position\"] = home_players.iloc[i - 1][\"position\"]\n",
    "        game_row[f\"home_player_{i}_off_rating\"] = home_players.iloc[i - 1][\"offensive_rating\"]\n",
    "        game_row[f\"home_player_{i}_def_rating\"] = home_players.iloc[i - 1][\"defensive_rating\"]\n",
    "        game_row[f\"home_player_{i}_usage\"] = home_players.iloc[i - 1][\"usage_percentage\"]\n",
    "    else:\n",
    "        game_row[f\"home_player_{i}_id\"] = None\n",
    "        game_row[f\"home_player_{i}_name\"] = None\n",
    "        game_row[f\"home_player_{i}_minutes\"] = None\n",
    "        game_row[f\"home_player_{i}_position\"] = None\n",
    "        game_row[f\"home_player_{i}_off_rating\"] = None\n",
    "        game_row[f\"home_player_{i}_def_rating\"] = None\n",
    "        game_row[f\"home_player_{i}_usage\"] = None\n",
    "\n",
    "    if i <= len(away_players):\n",
    "        game_row[f\"away_player_{i}_id\"] = away_players.iloc[i - 1][\"player_id\"]\n",
    "        game_row[f\"away_player_{i}_name\"] = away_players.iloc[i - 1][\"full_name\"]\n",
    "        game_row[f\"away_player_{i}_minutes\"] = away_players.iloc[i - 1][\"minutes_played\"]\n",
    "        game_row[f\"away_player_{i}_position\"] = away_players.iloc[i - 1][\"position\"]\n",
    "        game_row[f\"away_player_{i}_off_rating\"] = away_players.iloc[i - 1][\"offensive_rating\"]\n",
    "        game_row[f\"away_player_{i}_def_rating\"] = away_players.iloc[i - 1][\"defensive_rating\"]\n",
    "        game_row[f\"away_player_{i}_usage\"] = away_players.iloc[i - 1][\"usage_percentage\"]\n",
    "    else:\n",
    "        game_row[f\"away_player_{i}_id\"] = None\n",
    "        game_row[f\"away_player_{i}_name\"] = None\n",
    "        game_row[f\"away_player_{i}_minutes\"] = None\n",
    "        game_row[f\"away_player_{i}_position\"] = None\n",
    "        game_row[f\"away_player_{i}_off_rating\"] = None\n",
    "        game_row[f\"away_player_{i}_def_rating\"] = None\n",
    "        game_row[f\"away_player_{i}_usage\"] = None\n",
    "\n",
    "# Create final DataFrame\n",
    "final_df = pd.DataFrame([game_row])\n",
    "\n",
    "# Save the data to a CSV file\n",
    "output_file = \"miniexp_games_2003_2023_top12_with_positions.csv\"\n",
    "final_df.to_csv(output_file, index=False)\n",
    "print(f\"Data saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'id' renamed to 'game_id'.\n",
      "Updated dataset saved as expanded_games_2003_2023.csv.\n"
     ]
    }
   ],
   "source": [
    "# Load the existing games DataFrame, and expand with our new categories ready to be filled in the next step\n",
    "games_df = pd.read_csv(\"games_2003_2023.csv\")\n",
    "\n",
    "# Define the player-specific columns for home and away teams, including position\n",
    "player_columns = []\n",
    "for i in range(1, 13):\n",
    "    player_columns.extend([\n",
    "        f\"home_player_{i}_id\", f\"home_player_{i}_name\", f\"home_player_{i}_position\",\n",
    "        f\"home_player_{i}_minutes\", f\"home_player_{i}_off_rating\",\n",
    "        f\"home_player_{i}_def_rating\", f\"home_player_{i}_usage\",\n",
    "    ])\n",
    "for i in range(1, 13):\n",
    "    player_columns.extend([\n",
    "        f\"away_player_{i}_id\", f\"away_player_{i}_name\", f\"away_player_{i}_position\",\n",
    "        f\"away_player_{i}_minutes\", f\"away_player_{i}_off_rating\",\n",
    "        f\"away_player_{i}_def_rating\", f\"away_player_{i}_usage\",\n",
    "    ])\n",
    "\n",
    "# Create an empty DataFrame for player columns\n",
    "empty_player_df = pd.DataFrame(columns=player_columns)\n",
    "\n",
    "# Initialize all values as None\n",
    "for col in empty_player_df.columns:\n",
    "    empty_player_df[col] = None\n",
    "\n",
    "# Append the empty player DataFrame to games_df\n",
    "games_df = pd.concat([games_df, empty_player_df], axis=1)\n",
    "\n",
    "# Rename the `id` column to `game_id`\n",
    "if 'id' in games_df.columns:\n",
    "    games_df.rename(columns={'id': 'game_id'}, inplace=True)\n",
    "    print(\"Column 'id' renamed to 'game_id'.\")\n",
    "else:\n",
    "    print(\"'id' column not found. Ensure the dataset is correct.\")\n",
    "\n",
    "# Save the updated dataset\n",
    "expanded_games_file_updated = \"expanded_games_2003_2023.csv\"\n",
    "games_df.to_csv(expanded_games_file_updated, index=False)\n",
    "print(f\"Updated dataset saved as {expanded_games_file_updated}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the expanded games DataFrame with statistics from BallDontLie utilizing a threaded approach for speed. See above (single game example - but in a loop).\n",
    "expanded_games_file = \"expanded_games_2003_2023.csv\"\n",
    "if os.path.exists(expanded_games_file):\n",
    "    games_df = pd.read_csv(expanded_games_file)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"{expanded_games_file} not found.\")\n",
    "\n",
    "# Identify unprocessed games\n",
    "unprocessed_games = games_df[games_df[\"home_player_1_id\"].isna()][\"game_id\"].tolist()\n",
    "print(f\"Found {len(unprocessed_games)} unprocessed games.\")\n",
    "\n",
    "# Set threading and batch processing parameters\n",
    "lock = threading.Lock()\n",
    "processed_games = 0\n",
    "batch_size = 100\n",
    "\n",
    "def process_game(game_id):\n",
    "    try:\n",
    "        # print(f\"Processing game ID {game_id}...\")\n",
    "\n",
    "        # Fetch stats for the game\n",
    "        base_stats = fetch_stats(BASE_URL_STATS, game_id)\n",
    "        advanced_stats = fetch_stats(BASE_URL_ADVANCED, game_id)\n",
    "\n",
    "        if not base_stats or not advanced_stats:\n",
    "            print(f\"No stats found for game ID {game_id}. Skipping...\")\n",
    "            return None, game_id\n",
    "\n",
    "        # Convert to DataFrames\n",
    "        base_df = pd.DataFrame(base_stats)\n",
    "        adv_df = pd.DataFrame(advanced_stats)\n",
    "\n",
    "        # Parse minutes played\n",
    "        base_df[\"minutes_played\"] = base_df[\"min\"].apply(parse_minutes)\n",
    "\n",
    "        # Merge base and advanced stats on player ID\n",
    "        base_df[\"player_id\"] = base_df[\"player\"].apply(lambda x: x[\"id\"])\n",
    "        adv_df[\"player_id\"] = adv_df[\"player\"].apply(lambda x: x[\"id\"])\n",
    "        adv_df[\"position\"] = adv_df[\"player\"].apply(lambda x: x.get(\"position\", None))  # Extract position\n",
    "        merged_df = pd.merge(\n",
    "            base_df,\n",
    "            adv_df[[\"player_id\", \"offensive_rating\", \"defensive_rating\", \"usage_percentage\", \"position\"]],\n",
    "            on=\"player_id\",\n",
    "            how=\"inner\"\n",
    "        )\n",
    "\n",
    "        # Add team and player full name\n",
    "        merged_df[\"team_id\"] = merged_df[\"team\"].apply(lambda x: x[\"id\"])\n",
    "        merged_df[\"team_name\"] = merged_df[\"team\"].apply(lambda x: x[\"full_name\"])\n",
    "        merged_df[\"full_name\"] = merged_df[\"player\"].apply(lambda x: f\"{x['first_name']} {x['last_name']}\")\n",
    "\n",
    "        # Extract home and away team IDs from the main DataFrame\n",
    "        home_team_id = games_df.loc[games_df[\"game_id\"] == game_id, \"home_team_id\"].values[0]\n",
    "        away_team_id = games_df.loc[games_df[\"game_id\"] == game_id, \"visitor_team_id\"].values[0]\n",
    "\n",
    "        # Split into home and away players and select top 12 by minutes played\n",
    "        home_players = merged_df[merged_df[\"team_id\"] == home_team_id].nlargest(12, \"minutes_played\")\n",
    "        away_players = merged_df[merged_df[\"team_id\"] == away_team_id].nlargest(12, \"minutes_played\")\n",
    "\n",
    "        # Create a dictionary to store processed player data\n",
    "        player_data = {}\n",
    "        for i in range(1, 13):\n",
    "            for team, players in [(\"home\", home_players), (\"away\", away_players)]:\n",
    "                if i <= len(players):\n",
    "                    player_data[f\"{team}_player_{i}_id\"] = players.iloc[i - 1][\"player_id\"]\n",
    "                    player_data[f\"{team}_player_{i}_name\"] = players.iloc[i - 1][\"full_name\"]\n",
    "                    player_data[f\"{team}_player_{i}_minutes\"] = players.iloc[i - 1][\"minutes_played\"]\n",
    "                    player_data[f\"{team}_player_{i}_position\"] = players.iloc[i - 1][\"position\"]\n",
    "                    player_data[f\"{team}_player_{i}_off_rating\"] = players.iloc[i - 1][\"offensive_rating\"]\n",
    "                    player_data[f\"{team}_player_{i}_def_rating\"] = players.iloc[i - 1][\"defensive_rating\"]\n",
    "                    player_data[f\"{team}_player_{i}_usage\"] = players.iloc[i - 1][\"usage_percentage\"]\n",
    "                else:\n",
    "                    player_data[f\"{team}_player_{i}_id\"] = None\n",
    "                    player_data[f\"{team}_player_{i}_name\"] = None\n",
    "                    player_data[f\"{team}_player_{i}_minutes\"] = None\n",
    "                    player_data[f\"{team}_player_{i}_position\"] = None\n",
    "                    player_data[f\"{team}_player_{i}_off_rating\"] = None\n",
    "                    player_data[f\"{team}_player_{i}_def_rating\"] = None\n",
    "                    player_data[f\"{team}_player_{i}_usage\"] = None\n",
    "\n",
    "        # print(f\"Finished processing game ID {game_id}.\")\n",
    "        return player_data, game_id\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing game {game_id}: {e}\")\n",
    "        return None, game_id\n",
    "# Function to update the DataFrame with processed game data\n",
    "def update_games_df(game_data, game_id):\n",
    "    global games_df\n",
    "    with lock:\n",
    "        for key, value in game_data.items():\n",
    "            games_df.loc[games_df[\"game_id\"] == game_id, key] = value\n",
    "\n",
    "# Function to save progress to a file\n",
    "def save_progress():\n",
    "    global games_df\n",
    "    with lock:\n",
    "        games_df.to_csv(expanded_games_file, index=False)\n",
    "        print(f\"Progress saved at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# ThreadPoolExecutor for processing games\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = executor.map(process_game, unprocessed_games)\n",
    "    for game_data, game_id in futures:\n",
    "        if game_data:\n",
    "            update_games_df(game_data, game_id)\n",
    "            processed_games += 1\n",
    "            if processed_games % batch_size == 0:\n",
    "                save_progress()\n",
    "\n",
    "# Final save\n",
    "save_progress()\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Accomplishments\n",
    "\n",
    "- Successfully retrieved and processed **~27,000 NBA games** across two decades.\n",
    "- Incorporated detailed player and game statistics into a comprehensive dataset for deep learning.\n",
    "- Addressed challenges with missing data, API rate limits, and processing time through incremental saving and parallelization.\n",
    "- Created a well-structured dataset with features like **player usage, minutes played, and team compositions**, enabling future modeling efforts.\n",
    "- This framework allows for a good jumping off point - we can incorporate more detailed player statistics (e.g. height) if wanted through a quick API access.\n",
    "\n",
    "This dataset now provides a solid foundation for training the transformer-based deep learning model in subsequent sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved static tensors for game 15486 in season 2003: Home -> static_game_tensors/season_2003/15486_home_static.pt, Away -> static_game_tensors/season_2003/15486_away_static.pt\n"
     ]
    }
   ],
   "source": [
    "# Creating a tensor for a single game example\n",
    "\n",
    "# File for player height and weight lookup\n",
    "PLAYER_BIO_FILE = \"playerBio.csv\"\n",
    "\n",
    "# Directory for saving static tensors\n",
    "TENSOR_DIR = \"static_game_tensors\"\n",
    "os.makedirs(TENSOR_DIR, exist_ok=True)\n",
    "\n",
    "# Create the player bio file if it doesn't exist\n",
    "if not os.path.exists(PLAYER_BIO_FILE):\n",
    "    pd.DataFrame(columns=[\"player_id\", \"height\", \"weight\"]).to_csv(PLAYER_BIO_FILE, index=False)\n",
    "\n",
    "def fetch_or_lookup_player_stats(player_id):\n",
    "    \"\"\"\n",
    "    Fetch the player's height and weight from the lookup file or API.\n",
    "    \"\"\"\n",
    "    player_bio = pd.read_csv(PLAYER_BIO_FILE)\n",
    "    if player_id in player_bio[\"player_id\"].values:\n",
    "        player_row = player_bio[player_bio[\"player_id\"] == player_id].iloc[0]\n",
    "        return {\"height\": player_row[\"height\"], \"weight\": player_row[\"weight\"]}\n",
    "    else:\n",
    "        # Fallback for height/weight API lookup if not in the CSV\n",
    "        return {\"height\": 72, \"weight\": 200}  # Default values for height/weight\n",
    "\n",
    "def process_team_static(game_row, team_prefix):\n",
    "    \"\"\"\n",
    "    Process a team's static features for saving.\n",
    "    \"\"\"\n",
    "    players = []\n",
    "    for i in range(1, 13):\n",
    "        player_id = game_row[f\"{team_prefix}_player_{i}_id\"]\n",
    "        player_data = {\n",
    "            \"player_id\": int(player_id) if not pd.isna(player_id) else None,\n",
    "            \"minutes\": game_row[f\"{team_prefix}_player_{i}_minutes\"],\n",
    "            \"off_rating\": game_row[f\"{team_prefix}_player_{i}_off_rating\"],\n",
    "            \"def_rating\": game_row[f\"{team_prefix}_player_{i}_def_rating\"],\n",
    "            \"usage\": game_row[f\"{team_prefix}_player_{i}_usage\"],\n",
    "        }\n",
    "        if player_data[\"player_id\"] is not None:\n",
    "            stats = fetch_or_lookup_player_stats(player_data[\"player_id\"])\n",
    "            player_data.update(stats)\n",
    "        players.append(player_data)\n",
    "    \n",
    "    # Normalize static features\n",
    "    numeric_features = [\"minutes\", \"off_rating\", \"def_rating\", \"usage\", \"height\", \"weight\"]\n",
    "    scaler = MinMaxScaler()\n",
    "    numeric_data = [[player[f] for f in numeric_features] for player in players if player[\"player_id\"] is not None]\n",
    "    if len(numeric_data) > 0:\n",
    "        scaler.fit(numeric_data)\n",
    "        normalized_data = scaler.transform(numeric_data)\n",
    "    else:\n",
    "        normalized_data = []\n",
    "\n",
    "    team_tensor = []\n",
    "    for i, player in enumerate(players):\n",
    "        if player[\"player_id\"] is None:\n",
    "            continue\n",
    "        numeric_values = [player[f] if f in player and player[f] is not None else 0 for f in numeric_features]\n",
    "        normalized_features = scaler.transform([numeric_values])[0] if len(numeric_data) > 0 else [0] * len(numeric_features)\n",
    "        static_tensor = torch.tensor(normalized_features, dtype=torch.float32)\n",
    "        team_tensor.append(static_tensor)\n",
    "    \n",
    "    # Pad to ensure 12 players\n",
    "    while len(team_tensor) < 12:\n",
    "        team_tensor.append(torch.zeros_like(team_tensor[0]))\n",
    "\n",
    "    return torch.stack(team_tensor)\n",
    "\n",
    "def save_static_tensors(game_row, home_tensor, away_tensor):\n",
    "    \"\"\"\n",
    "    Save home and away static tensors for a game in season-specific folders.\n",
    "    \"\"\"\n",
    "    season = game_row[\"season\"]  # Extract season\n",
    "    season_dir = os.path.join(TENSOR_DIR, f\"season_{season}\")\n",
    "    os.makedirs(season_dir, exist_ok=True)  # Create season directory if it doesn't exist\n",
    "\n",
    "    # Save tensors\n",
    "    game_id = game_row[\"game_id\"]\n",
    "    home_path = os.path.join(season_dir, f\"{game_id}_home_static.pt\")\n",
    "    away_path = os.path.join(season_dir, f\"{game_id}_away_static.pt\")\n",
    "    torch.save(home_tensor, home_path)\n",
    "    torch.save(away_tensor, away_path)\n",
    "    print(f\"Saved static tensors for game {game_id} in season {season}: Home -> {home_path}, Away -> {away_path}\")\n",
    "\n",
    "# Example: Process and save static tensors for a single game\n",
    "game_row = games_df[games_df[\"game_id\"] == game_id].iloc[0]\n",
    "home_static_tensor = process_team_static(game_row, \"home\")\n",
    "away_static_tensor = process_team_static(game_row, \"away\")\n",
    "save_static_tensors(game_row, home_static_tensor, away_static_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going over entire data set to create static tensors for each game_id for 12xHome Players and 12xAway Players - saved and organized in static_game_tensors - at runtime will be linked with dynamic encodings (playerID,position,teamID)\n",
    "PLAYER_BIO_FILE = \"playerBio.csv\"\n",
    "\n",
    "# Directory for saving static tensors\n",
    "TENSOR_DIR = \"static_game_tensors\"\n",
    "os.makedirs(TENSOR_DIR, exist_ok=True)\n",
    "\n",
    "# Create the player bio file if it doesn't exist\n",
    "if not os.path.exists(PLAYER_BIO_FILE):\n",
    "    pd.DataFrame(columns=[\"player_id\", \"height\", \"weight\"]).to_csv(PLAYER_BIO_FILE, index=False)\n",
    "\n",
    "def fetch_or_lookup_player_stats(player_id):\n",
    "    \"\"\"\n",
    "    Fetch the player's height and weight from the lookup file or API.\n",
    "    \"\"\"\n",
    "    player_bio = pd.read_csv(PLAYER_BIO_FILE)\n",
    "    if player_id in player_bio[\"player_id\"].values:\n",
    "        player_row = player_bio[player_bio[\"player_id\"] == player_id].iloc[0]\n",
    "        return {\"height\": player_row[\"height\"], \"weight\": player_row[\"weight\"]}\n",
    "    else:\n",
    "        return {\"height\": 72, \"weight\": 200}  # Default values\n",
    "\n",
    "def process_team_static(game_row, team_prefix):\n",
    "    \"\"\"\n",
    "    Process a team's static features for saving.\n",
    "    \"\"\"\n",
    "    players = []\n",
    "    for i in range(1, 13):\n",
    "        player_id = game_row[f\"{team_prefix}_player_{i}_id\"]\n",
    "        player_data = {\n",
    "            \"player_id\": int(player_id) if not pd.isna(player_id) else None,\n",
    "            \"minutes\": game_row[f\"{team_prefix}_player_{i}_minutes\"],\n",
    "            \"off_rating\": game_row[f\"{team_prefix}_player_{i}_off_rating\"],\n",
    "            \"def_rating\": game_row[f\"{team_prefix}_player_{i}_def_rating\"],\n",
    "            \"usage\": game_row[f\"{team_prefix}_player_{i}_usage\"],\n",
    "        }\n",
    "        if player_data[\"player_id\"] is not None:\n",
    "            stats = fetch_or_lookup_player_stats(player_data[\"player_id\"])\n",
    "            player_data.update(stats)\n",
    "        players.append(player_data)\n",
    "\n",
    "    # Normalize static features\n",
    "    numeric_features = [\"minutes\", \"off_rating\", \"def_rating\", \"usage\", \"height\", \"weight\"]\n",
    "    scaler = MinMaxScaler()\n",
    "    numeric_data = [[player[f] for f in numeric_features] for player in players if player[\"player_id\"] is not None]\n",
    "    if len(numeric_data) > 0:\n",
    "        scaler.fit(numeric_data)\n",
    "        normalized_data = scaler.transform(numeric_data)\n",
    "    else:\n",
    "        normalized_data = []\n",
    "\n",
    "    team_tensor = []\n",
    "    for i, player in enumerate(players):\n",
    "        if player[\"player_id\"] is None:\n",
    "            continue\n",
    "        numeric_values = [player[f] if f in player and player[f] is not None else 0 for f in numeric_features]\n",
    "        normalized_features = scaler.transform([numeric_values])[0] if len(numeric_data) > 0 else [0] * len(numeric_features)\n",
    "        static_tensor = torch.tensor(normalized_features, dtype=torch.float32)\n",
    "        team_tensor.append(static_tensor)\n",
    "\n",
    "    # Sort players by playtime (minutes played) in descending order\n",
    "    team_tensor = [tensor for _, tensor in sorted(zip(numeric_data, team_tensor), key=lambda x: x[0][0], reverse=True)]\n",
    "\n",
    "    # Pad to ensure 12 players\n",
    "    while len(team_tensor) < 12:\n",
    "        team_tensor.append(torch.zeros_like(team_tensor[0]))\n",
    "\n",
    "    return torch.stack(team_tensor)\n",
    "\n",
    "def save_static_tensors(game_row):\n",
    "    \"\"\"\n",
    "    Save home and away static tensors for a game in season-specific folders.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract season and game ID\n",
    "        season = game_row[\"season\"]\n",
    "        game_id = game_row[\"game_id\"]\n",
    "\n",
    "        # Skip rows with invalid critical values\n",
    "        if pd.isna(season) or pd.isna(game_id) or pd.isna(game_row[\"home_team_id\"]) or pd.isna(game_row[\"visitor_team_id\"]):\n",
    "            print(f\"Skipping invalid game data: Game ID {game_id}\")\n",
    "            return\n",
    "\n",
    "        # Process home and away teams\n",
    "        home_static_tensor = process_team_static(game_row, \"home\")\n",
    "        away_static_tensor = process_team_static(game_row, \"away\")\n",
    "\n",
    "        # Save tensors\n",
    "        season_dir = os.path.join(TENSOR_DIR, f\"season_{season}\")\n",
    "        os.makedirs(season_dir, exist_ok=True)\n",
    "        home_path = os.path.join(season_dir, f\"{game_id}_home_static.pt\")\n",
    "        away_path = os.path.join(season_dir, f\"{game_id}_away_static.pt\")\n",
    "        torch.save(home_static_tensor, home_path)\n",
    "        torch.save(away_static_tensor, away_path)\n",
    "        print(f\"Saved static tensors for game {game_id} in season {season}: Home -> {home_path}, Away -> {away_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing game {game_row['game_id'] if 'game_id' in game_row else 'unknown'}: {e}\")\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = \"expanded_games_2003_2023.csv\"\n",
    "try:\n",
    "    games_df = pd.read_csv(dataset_path)\n",
    "    print(f\"Dataset '{dataset_path}' loaded successfully. Number of rows: {len(games_df)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{dataset_path}' not found.\")\n",
    "    raise\n",
    "\n",
    "# Loop through all games and save tensors\n",
    "for _, game_row in games_df.iterrows():\n",
    "    save_static_tensors(game_row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **II. Model Creation**\n",
    "\n",
    "### Overview\n",
    "This section focuses on building a transformer-based deep learning model to predict NBA game outcomes. The model will analyze static player features and dynamically learned embeddings to generate predictions.\n",
    "\n",
    "### Goals\n",
    "1. **Model Architecture**:\n",
    "   - Implement a transformer network using **PyTorch**.\n",
    "   - Use multi-head attention mechanisms to analyze player-to-player and team-to-team relationships.\n",
    "   - Experiment with architectures that include residual connections to enhance model depth and stability.\n",
    "\n",
    "2. **Input and Output Design**:\n",
    "   - **Inputs**:\n",
    "     - Static tensors for each team, including player statistics normalized per game.\n",
    "     - Dynamically learned embeddings for PlayerID, PositionID, TeamID, and Season, added at runtime.\n",
    "   - **Outputs**:\n",
    "     - Predict the final scores for the home and away teams.\n",
    "\n",
    "3. **Model Training**:\n",
    "   - Train the model chronologically, using data from past games to predict future ones.\n",
    "   - Define the training loop with appropriate loss functions and optimizers.\n",
    "   - Split data into training, validation, and test sets based on game chronology for temporal consistency.\n",
    "\n",
    "### Implementation Steps\n",
    "1. **Set Up Dynamic Embeddings**:\n",
    "   - Initialize embeddings for PlayerID, PositionID, TeamID, and Season, which will be added dynamically to the static tensors at runtime.\n",
    "\n",
    "2. **Combine Static and Dynamic Features**:\n",
    "   - Design a process to concatenate static and dynamic features for each player token during training and inference.\n",
    "\n",
    "3. **Define the Transformer Architecture**:\n",
    "   - Specify input dimensions, number of attention heads, and transformer layers.\n",
    "   - Experiment with designs, including multi-head attention blocks and fully connected layers.\n",
    "\n",
    "4. **Configure the Training Pipeline**:\n",
    "   - Choose a regression-based loss function (e.g., Mean Squared Error) and an optimizer (e.g., Adam).\n",
    "   - Set hyperparameters such as learning rate, batch size, and number of epochs.\n",
    "\n",
    "5. **Initial Testing**:\n",
    "   - Train the model on a subset of the data (e.g., a single season) to validate functionality and debug issues.\n",
    "   - Evaluate performance metrics (e.g., MSE, RMSE) and refine the architecture.\n",
    "\n",
    "6. **Scaling and Tuning**:\n",
    "   - Train on the full dataset after initial testing.\n",
    "   - Perform hyperparameter tuning to optimize model performance.\n",
    "\n",
    "---\n",
    "\n",
    "This section will document the detailed process of creating and implementing the model, highlighting the reasoning behind architectural and design decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 640000000064 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10284/2149557688.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Define embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mplayer_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_player_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_player\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mposition_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_position_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mteam_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_team_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_team\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             self.weight = Parameter(\n\u001b[0;32m--> 167\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m                 \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0m_freeze\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 640000000064 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "# Embedding dimensions - Example of creating a player token with their dynamic and static features combined, the whole team will be a tensor\n",
    "E_player = 16\n",
    "E_position = 4\n",
    "E_team = 8\n",
    "E_season = 4\n",
    "\n",
    "# Maximum values for embedding ranges\n",
    "max_player_id = 2500  # Example max number of players\n",
    "max_team_id = 33  # Including Supersonics\n",
    "max_position_id = 3  # C, F, G, None -> 0, 1, 2, 3\n",
    "max_season_id = 20  # Seasons from 2003 to 2023 -> 0 to 20\n",
    "\n",
    "# Define embeddings\n",
    "player_embedding = nn.Embedding(max_player_id + 1, E_player)\n",
    "position_embedding = nn.Embedding(max_position_id + 1, E_position)\n",
    "team_embedding = nn.Embedding(max_team_id + 1, E_team)\n",
    "season_embedding = nn.Embedding(max_season_id + 1, E_season)\n",
    "\n",
    "# Example function to create a full player token\n",
    "def create_player_token(player_id, position_id, team_id_for, team_id_against, season_id, static_tensor):\n",
    "    \"\"\"\n",
    "    Concatenate dynamic embeddings with static features to create a full player token.\n",
    "    \"\"\"\n",
    "    # Dynamic embeddings\n",
    "    player_id_tensor = player_embedding(torch.tensor(player_id, dtype=torch.long))\n",
    "    position_tensor = position_embedding(torch.tensor(position_id, dtype=torch.long))\n",
    "    team_for_tensor = team_embedding(torch.tensor(team_id_for, dtype=torch.long))\n",
    "    team_against_tensor = team_embedding(torch.tensor(team_id_against, dtype=torch.long))\n",
    "    season_tensor = season_embedding(torch.tensor(season_id, dtype=torch.long))\n",
    "    \n",
    "    # Concatenate dynamic and static features\n",
    "    player_token = torch.cat([\n",
    "        player_id_tensor, position_tensor, team_for_tensor, team_against_tensor, season_tensor, static_tensor\n",
    "    ])\n",
    "    return player_token\n",
    "\n",
    "# Example of combining with a static tensor\n",
    "static_tensor_example = torch.tensor([1.0, 0.9, 0.8, 0.7, 0.6, 0.5], dtype=torch.float32)  # Example static features\n",
    "player_token_example = create_player_token(\n",
    "    player_id=23,  # Example player ID\n",
    "    position_id=2,  # Guard\n",
    "    team_id_for=15,  # Example team ID\n",
    "    team_id_against=20,  # Example opponent team ID\n",
    "    season_id=10,  # Example season (2013)\n",
    "    static_tensor=static_tensor_example\n",
    ")\n",
    "print(f\"Player Token Shape: {player_token_example.shape}\")\n",
    "print(f\"Player Token Data:\\n{player_token_example}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (replace the file path with your actual path)\n",
    "file_path = \"expanded_games_2003_2023.csv\"  # Update this\n",
    "games_df = pd.read_csv(file_path)\n",
    "\n",
    "# Extract player ID and name columns (for both home and away players)\n",
    "player_columns = []\n",
    "for i in range(1, 13):\n",
    "    player_columns.extend([\n",
    "        f\"home_player_{i}_id\",\n",
    "        f\"home_player_{i}_name\",\n",
    "        f\"away_player_{i}_id\",\n",
    "        f\"away_player_{i}_name\"\n",
    "    ])\n",
    "\n",
    "# Filter only the player-related columns\n",
    "player_data = games_df[player_columns]\n",
    "\n",
    "# Reshape the data for easier inspection (stack IDs and names together)\n",
    "player_data_melted = pd.DataFrame({\n",
    "    \"Player ID\": player_data.filter(like=\"_id\").stack().values,\n",
    "    \"Player Name\": player_data.filter(like=\"_name\").stack().values\n",
    "})\n",
    "\n",
    "# Remove duplicates for a concise view of unique player data\n",
    "unique_player_data = player_data_melted.drop_duplicates()\n",
    "\n",
    "# Save the unique player data to a CSV file (optional)\n",
    "unique_player_data.to_csv(\"unique_player_data.csv\", index=False)\n",
    "\n",
    "# Display the first few rows\n",
    "print(unique_player_data.head(20))\n",
    "\n",
    "\n",
    "# Load the unique player data\n",
    "unique_player_file = \"unique_player_data.csv\"  # Replace with your file path\n",
    "unique_player_data = pd.read_csv(unique_player_file)\n",
    "\n",
    "# Ensure only unique player IDs\n",
    "unique_player_data = unique_player_data.drop_duplicates(subset=[\"Player ID\"])\n",
    "\n",
    "# Generate a mapping for normalization\n",
    "normalized_range = range(0, 2500)\n",
    "player_id_mapping = {\n",
    "    original_id: normalized_id\n",
    "    for original_id, normalized_id in zip(unique_player_data[\"Player ID\"], normalized_range)\n",
    "}\n",
    "\n",
    "# Add a normalized ID column to the DataFrame\n",
    "unique_player_data[\"Normalized ID\"] = unique_player_data[\"Player ID\"].map(player_id_mapping)\n",
    "\n",
    "# Save the mapping for future use\n",
    "mapping_file = \"player_id_mapping_normalized.csv\"\n",
    "unique_player_data.to_csv(mapping_file, index=False)\n",
    "\n",
    "print(f\"Player ID mapping saved to {mapping_file}\")\n",
    "print(unique_player_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home Tokens Shape: torch.Size([12, 48])\n",
      "Away Tokens Shape: torch.Size([12, 48])\n",
      "Home Token 0:\n",
      "tensor([-1.8169,  0.1873, -0.9661, -1.4261, -0.0202, -0.7314, -0.3624,  1.1713,\n",
      "         0.6929,  0.4051, -0.5035, -0.9490, -0.6739,  0.0880, -0.0351,  0.0582,\n",
      "         0.3637,  0.7658,  1.5315, -1.5421,  0.9381, -1.3416, -0.4498,  1.8464,\n",
      "         0.2502,  0.3253,  0.7020,  0.1501,  0.8423,  0.4031,  1.6838,  1.8911,\n",
      "         1.1788,  0.7227,  0.5706,  1.1671,  2.4628,  0.3548, -0.1746,  0.7888,\n",
      "         1.2443,  0.5755,  1.0000,  0.9069,  0.7266,  0.7092,  0.0000,  0.1786],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Home Token 1:\n",
      "tensor([ 0.8535, -0.3570,  0.5979, -0.4216, -0.6281,  0.6064, -1.6078,  1.7100,\n",
      "         2.2328,  1.9776, -0.7605, -0.1966, -0.5590, -1.6446, -0.7251, -1.2465,\n",
      "        -1.0922,  0.3917,  1.5315, -1.5421,  0.9381, -1.3416, -0.4498,  1.8464,\n",
      "         0.2502,  0.3253,  0.7020,  0.1501,  0.8423,  0.4031,  1.6838,  1.8911,\n",
      "         1.1788,  0.7227,  0.5706,  1.1671,  2.4628,  0.3548, -0.1746,  0.7888,\n",
      "         1.2443,  0.5755,  0.9651,  0.9735,  0.7326,  0.9078,  0.2500,  0.0357],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Home Token 2:\n",
      "tensor([-1.8689, -0.8373,  0.8764, -0.7136, -0.1672,  0.6192,  0.9665, -1.9162,\n",
      "         0.1947, -0.9242,  0.4997,  0.6953, -0.8252, -1.0974,  0.8209, -0.3065,\n",
      "        -1.7677, -0.4137,  1.5315, -1.5421,  0.9381, -1.3416, -0.4498,  1.8464,\n",
      "         0.2502,  0.3253,  0.7020,  0.1501,  0.8423,  0.4031,  1.6838,  1.8911,\n",
      "         1.1788,  0.7227,  0.5706,  1.1671,  2.4628,  0.3548, -0.1746,  0.7888,\n",
      "         1.2443,  0.5755,  0.9272,  0.9029,  0.8089,  0.4043,  0.5833,  0.3571],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Home Token 3:\n",
      "tensor([-0.6115,  1.4122, -0.0500, -0.7080,  0.6677,  0.9857,  0.0398,  0.1994,\n",
      "        -0.0794, -0.5451,  0.6061,  0.0455,  0.1273, -0.7983,  0.7588, -0.5598,\n",
      "        -0.0474,  0.0486,  1.5315, -1.5421,  0.9381, -1.3416, -0.4498,  1.8464,\n",
      "         0.2502,  0.3253,  0.7020,  0.1501,  0.8423,  0.4031,  1.6838,  1.8911,\n",
      "         1.1788,  0.7227,  0.5706,  1.1671,  2.4628,  0.3548, -0.1746,  0.7888,\n",
      "         1.2443,  0.5755,  0.8579,  0.9133,  0.7515,  1.0000,  1.0000,  1.0000],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Home Token 4:\n",
      "tensor([-0.0355, -0.8562,  1.1960, -0.2342,  0.8750,  0.0201, -0.8746,  1.1311,\n",
      "         1.6748, -1.0625,  0.5108,  0.7430, -0.6905, -0.0326,  1.2373,  2.2537,\n",
      "        -0.7362, -0.2091,  1.5315, -1.5421,  0.9381, -1.3416, -0.4498,  1.8464,\n",
      "         0.2502,  0.3253,  0.7020,  0.1501,  0.8423,  0.4031,  1.6838,  1.8911,\n",
      "         1.1788,  0.7227,  0.5706,  1.1671,  2.4628,  0.3548, -0.1746,  0.7888,\n",
      "         1.2443,  0.5755,  0.7775,  1.0000,  0.7232,  0.6986,  0.6667,  0.5286],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Home Token 5:\n",
      "tensor([ 0.0912,  0.0954, -1.1856,  0.7521,  0.9739,  0.4510, -0.0710,  0.6410,\n",
      "         0.7897,  0.5992, -0.5104, -1.9318,  1.5465,  0.0854,  0.8879, -0.8699,\n",
      "        -0.1848, -0.0085,  1.5315, -1.5421,  0.9381, -1.3416, -0.4498,  1.8464,\n",
      "         0.2502,  0.3253,  0.7020,  0.1501,  0.8423,  0.4031,  1.6838,  1.8911,\n",
      "         1.1788,  0.7227,  0.5706,  1.1671,  2.4628,  0.3548, -0.1746,  0.7888,\n",
      "         1.2443,  0.5755,  0.5572,  0.7841,  0.8380,  0.6950,  0.5000,  0.2857],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Home Token 6:\n",
      "tensor([ 0.5523,  0.0626, -1.0005, -0.6644,  0.4314, -0.3247, -1.2586, -1.3693,\n",
      "         1.7807,  0.1398,  0.2764, -0.3896, -1.3310, -0.3565, -0.5628, -0.3353,\n",
      "         1.8013,  0.6460,  1.5315, -1.5421,  0.9381, -1.3416, -0.4498,  1.8464,\n",
      "         0.2502,  0.3253,  0.7020,  0.1501,  0.8423,  0.4031,  1.6838,  1.8911,\n",
      "         1.1788,  0.7227,  0.5706,  1.1671,  2.4628,  0.3548, -0.1746,  0.7888,\n",
      "         1.2443,  0.5755,  0.5362,  0.7841,  0.8569,  0.2234,  0.7500,  0.4286],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Home Token 7:\n",
      "tensor([ 1.1844, -0.8488,  0.2902,  1.4972,  0.7656, -0.1945,  0.2816, -0.2867,\n",
      "         0.2605,  0.0911,  0.7685, -0.2274, -0.5024,  0.8025,  1.1452,  0.0791,\n",
      "         0.7080,  0.8490,  1.5315, -1.5421,  0.9381, -1.3416, -0.4498,  1.8464,\n",
      "         0.2502,  0.3253,  0.7020,  0.1501,  0.8423,  0.4031,  1.6838,  1.8911,\n",
      "         1.1788,  0.7227,  0.5706,  1.1671,  2.4628,  0.3548, -0.1746,  0.7888,\n",
      "         1.2443,  0.5755,  0.3753,  0.7472,  0.9751,  0.6879,  0.4167,  0.2143],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Home Token 8:\n",
      "tensor([-0.0390,  0.1055,  2.3306,  1.1441,  0.2084,  0.3520,  0.6625,  1.3236,\n",
      "        -0.8779,  0.8248,  1.2138,  0.2836,  0.2051, -1.0428, -0.3952, -0.3682,\n",
      "        -2.2340, -0.1295,  1.5315, -1.5421,  0.9381, -1.3416, -0.4498,  1.8464,\n",
      "         0.2502,  0.3253,  0.7020,  0.1501,  0.8423,  0.4031,  1.6838,  1.8911,\n",
      "         1.1788,  0.7227,  0.5706,  1.1671,  2.4628,  0.3548, -0.1746,  0.7888,\n",
      "         1.2443,  0.5755,  0.2413,  0.5490,  1.0000,  0.8865,  0.0000,  0.0000],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Home Token 9:\n",
      "tensor([-0.4939,  0.2698,  0.1735,  0.4399, -0.5076, -0.4147,  0.2035, -1.3026,\n",
      "         0.5161, -0.0843, -0.4607,  0.7879,  0.1958,  0.8087, -2.2792, -1.8576,\n",
      "        -0.7903,  0.7661,  1.5315, -1.5421,  0.9381, -1.3416, -0.4498,  1.8464,\n",
      "         0.2502,  0.3253,  0.7020,  0.1501,  0.8423,  0.4031,  1.6838,  1.8911,\n",
      "         1.1788,  0.7227,  0.5706,  1.1671,  2.4628,  0.3548, -0.1746,  0.7888,\n",
      "         1.2443,  0.5755,  0.1877,  0.3652,  0.6590,  0.8191,  0.5833,  0.3571],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Home Token 10:\n",
      "tensor([ 1.7698,  1.5001, -0.0265, -0.3416,  0.9365,  0.9978, -0.3910, -0.8418,\n",
      "         0.5757, -0.6461, -0.0294,  1.6173,  0.7376,  0.2869, -0.1714,  0.1467,\n",
      "         0.9648, -1.7920,  1.5315, -1.5421,  0.9381, -1.3416, -0.4498,  1.8464,\n",
      "         0.2502,  0.3253,  0.7020,  0.1501,  0.8423,  0.4031,  1.6838,  1.8911,\n",
      "         1.1788,  0.7227,  0.5706,  1.1671,  2.4628,  0.3548, -0.1746,  0.7888,\n",
      "         1.2443,  0.5755,  0.0000,  0.0000,  0.0000,  0.0000,  0.4167,  0.1929],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Home Token 11:\n",
      "tensor([ 2.4719, -0.8634,  1.0190,  0.4356, -0.0947, -2.1994, -0.2185, -0.5350,\n",
      "        -0.2265, -1.1964, -0.8878, -0.0026,  0.1622,  0.2537, -1.6566, -0.7666,\n",
      "        -0.4715,  0.9753,  1.5315, -1.5421,  0.9381, -1.3416, -0.4498,  1.8464,\n",
      "         0.2502,  0.3253,  0.7020,  0.1501,  0.8423,  0.4031,  1.6838,  1.8911,\n",
      "         1.1788,  0.7227,  0.5706,  1.1671,  2.4628,  0.3548, -0.1746,  0.7888,\n",
      "         1.2443,  0.5755,  0.0000,  0.0000,  0.0000,  0.0000,  0.7500,  0.4857],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Away Token 0:\n",
      "tensor([-0.2594,  0.5136,  1.7986,  1.2202,  0.6419, -0.9339,  0.1575, -0.4438,\n",
      "        -1.1285,  0.1527,  0.8247,  1.0594, -0.4832, -0.0544,  1.2872,  0.7917,\n",
      "         0.9409, -0.0066, -0.5424, -1.0365, -0.4652,  0.7120,  1.6838,  1.8911,\n",
      "         1.1788,  0.7227,  0.5706,  1.1671,  2.4628,  0.3548, -0.4498,  1.8464,\n",
      "         0.2502,  0.3253,  0.7020,  0.1501,  0.8423,  0.4031, -0.1746,  0.7888,\n",
      "         1.2443,  0.5755,  1.0000,  0.6000,  0.8216,  0.7818,  0.6842,  0.6907],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Away Token 1:\n",
      "tensor([-2.5392,  0.7191,  1.1385,  0.0960,  0.8912, -2.0149,  0.5456, -1.1010,\n",
      "        -0.4719, -0.9424,  0.3515, -0.3196,  1.3302,  0.3117,  0.8573, -0.5704,\n",
      "        -0.3590,  2.7640, -0.5424, -1.0365, -0.4652,  0.7120,  1.6838,  1.8911,\n",
      "         1.1788,  0.7227,  0.5706,  1.1671,  2.4628,  0.3548, -0.4498,  1.8464,\n",
      "         0.2502,  0.3253,  0.7020,  0.1501,  0.8423,  0.4031, -0.1746,  0.7888,\n",
      "         1.2443,  0.5755,  0.9937,  0.6913,  0.8501,  0.4052,  0.5263,  0.6907],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Away Token 2:\n",
      "tensor([ 1.3814,  0.9736,  1.3767,  0.3837, -0.1037, -2.1105, -0.3276, -0.3542,\n",
      "         0.4784, -0.1548, -0.0263, -0.0078,  0.1136, -0.4890,  0.1706,  0.6691,\n",
      "         0.0729,  0.7985,  1.5315, -1.5421,  0.9381, -1.3416,  1.6838,  1.8911,\n",
      "         1.1788,  0.7227,  0.5706,  1.1671,  2.4628,  0.3548, -0.4498,  1.8464,\n",
      "         0.2502,  0.3253,  0.7020,  0.1501,  0.8423,  0.4031, -0.1746,  0.7888,\n",
      "         1.2443,  0.5755,  0.9886,  0.9780,  0.6774,  0.3974,  0.5263,  0.5876],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Away Token 3:\n",
      "tensor([ 1.0386, -0.3267, -1.9678, -0.6807, -0.6743,  2.0508,  0.5056,  0.6351,\n",
      "         0.0920,  0.0785,  0.7385, -2.7350,  1.1163,  0.4842, -0.9330,  0.6497,\n",
      "        -1.3755,  0.9662,  1.5315, -1.5421,  0.9381, -1.3416,  1.6838,  1.8911,\n",
      "         1.1788,  0.7227,  0.5706,  1.1671,  2.4628,  0.3548, -0.4498,  1.8464,\n",
      "         0.2502,  0.3253,  0.7020,  0.1501,  0.8423,  0.4031, -0.1746,  0.7888,\n",
      "         1.2443,  0.5755,  0.8561,  0.7402,  0.7875,  0.4286,  0.4211,  0.4845],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Away Token 4:\n",
      "tensor([-0.0450,  1.6928, -0.1674,  0.1129,  1.0686,  0.8056,  0.9766,  0.9030,\n",
      "         0.0909, -0.2210, -0.4734,  0.2080,  0.0948, -1.0724, -0.7092, -0.4798,\n",
      "        -0.2920, -1.0334,  1.5315, -1.5421,  0.9381, -1.3416,  1.6838,  1.8911,\n",
      "         1.1788,  0.7227,  0.5706,  1.1671,  2.4628,  0.3548, -0.4498,  1.8464,\n",
      "         0.2502,  0.3253,  0.7020,  0.1501,  0.8423,  0.4031, -0.1746,  0.7888,\n",
      "         1.2443,  0.5755,  0.8224,  0.5858,  0.9848,  0.4519,  0.2105,  0.0000],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Away Token 5:\n",
      "tensor([ 0.8322,  0.5061, -1.5395,  0.1606,  0.6131,  1.7098, -1.1654,  0.8870,\n",
      "        -0.8273, -0.4264,  0.7099,  0.3681,  1.8903, -1.0231,  0.8874,  0.8303,\n",
      "        -1.5313,  0.2879,  1.5315, -1.5421,  0.9381, -1.3416,  1.6838,  1.8911,\n",
      "         1.1788,  0.7227,  0.5706,  1.1671,  2.4628,  0.3548, -0.4498,  1.8464,\n",
      "         0.2502,  0.3253,  0.7020,  0.1501,  0.8423,  0.4031, -0.1746,  0.7888,\n",
      "         1.2443,  0.5755,  0.8035,  0.8976,  0.6110,  0.3117,  0.1579,  0.1134],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Away Token 6:\n",
      "tensor([-0.7891,  0.3638,  1.0489, -0.4845,  1.2678, -0.0161,  1.1405, -0.3378,\n",
      "         0.8783,  0.0257,  0.0554,  0.1907, -1.0972,  0.5432, -0.6689, -0.4282,\n",
      "        -0.2356,  0.7125,  1.5315, -1.5421,  0.9381, -1.3416,  1.6838,  1.8911,\n",
      "         1.1788,  0.7227,  0.5706,  1.1671,  2.4628,  0.3548, -0.4498,  1.8464,\n",
      "         0.2502,  0.3253,  0.7020,  0.1501,  0.8423,  0.4031, -0.1746,  0.7888,\n",
      "         1.2443,  0.5755,  0.4112,  1.0000,  0.3264,  0.3273,  0.0000,  0.0412],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Away Token 7:\n",
      "tensor([-1.3780,  2.5677,  0.1304, -0.8500,  0.2636, -0.2430, -0.1193,  0.2531,\n",
      "         2.2077, -0.4419, -1.4860,  1.2922, -1.3252, -0.0307,  0.1508, -1.2465,\n",
      "         0.0305, -0.4221,  1.5315, -1.5421,  0.9381, -1.3416,  1.6838,  1.8911,\n",
      "         1.1788,  0.7227,  0.5706,  1.1671,  2.4628,  0.3548, -0.4498,  1.8464,\n",
      "         0.2502,  0.3253,  0.7020,  0.1501,  0.8423,  0.4031, -0.1746,  0.7888,\n",
      "         1.2443,  0.5755,  0.3427,  0.0000,  1.0000,  0.0000,  0.4737,  0.8454],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Away Token 8:\n",
      "tensor([ 0.7242,  0.6761, -0.6616, -0.3199, -1.3683, -0.2100,  0.0390, -0.4658,\n",
      "         0.3970,  0.6433, -1.4052, -0.3959, -0.4972, -0.0232, -0.3221, -0.7576,\n",
      "        -0.2891, -0.9340, -0.5424, -1.0365, -0.4652,  0.7120,  1.6838,  1.8911,\n",
      "         1.1788,  0.7227,  0.5706,  1.1671,  2.4628,  0.3548, -0.4498,  1.8464,\n",
      "         0.2502,  0.3253,  0.7020,  0.1501,  0.8423,  0.4031, -0.1746,  0.7888,\n",
      "         1.2443,  0.5755,  0.3084,  0.9906,  0.5427,  0.1844,  0.4737,  0.5876],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Away Token 9:\n",
      "tensor([-1.3642, -0.5066, -1.5563,  0.5221, -0.1591,  0.7561,  1.2578, -1.1513,\n",
      "         1.0048,  1.3369,  0.2457,  0.7124,  0.4875,  0.5484, -0.4042,  1.0242,\n",
      "        -0.6042,  0.5400,  1.5315, -1.5421,  0.9381, -1.3416,  1.6838,  1.8911,\n",
      "         1.1788,  0.7227,  0.5706,  1.1671,  2.4628,  0.3548, -0.4498,  1.8464,\n",
      "         0.2502,  0.3253,  0.7020,  0.1501,  0.8423,  0.4031, -0.1746,  0.7888,\n",
      "         1.2443,  0.5755,  0.0343,  0.7118,  0.3529,  1.0000,  0.4211,  0.3299],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Away Token 10:\n",
      "tensor([ 1.8260, -1.6357,  0.4339, -0.6567, -0.2934, -2.4696,  1.2283,  2.0608,\n",
      "         0.0293, -0.3477, -0.6704, -0.2778,  0.6868,  0.7625,  0.1314,  0.4637,\n",
      "         0.6341, -0.1869,  1.5315, -1.5421,  0.9381, -1.3416,  1.6838,  1.8911,\n",
      "         1.1788,  0.7227,  0.5706,  1.1671,  2.4628,  0.3548, -0.4498,  1.8464,\n",
      "         0.2502,  0.3253,  0.7020,  0.1501,  0.8423,  0.4031, -0.1746,  0.7888,\n",
      "         1.2443,  0.5755,  0.0000,  0.6724,  0.0000,  0.4364,  1.0000,  1.0000],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Away Token 11:\n",
      "tensor([-1.6579,  0.3776, -2.3484, -0.8495, -1.8756,  0.5963, -0.6969,  0.0840,\n",
      "        -0.2777, -1.4337, -2.8210, -0.8117, -0.7786, -1.0173,  0.1559, -0.8171,\n",
      "        -0.9193, -0.2763,  1.5315, -1.5421,  0.9381, -1.3416,  1.6838,  1.8911,\n",
      "         1.1788,  0.7227,  0.5706,  1.1671,  2.4628,  0.3548, -0.4498,  1.8464,\n",
      "         0.2502,  0.3253,  0.7020,  0.1501,  0.8423,  0.4031, -0.1746,  0.7888,\n",
      "         1.2443,  0.5755,  0.0000,  0.8693,  0.0000,  0.2052,  0.3684,  0.2268],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Static tensors directory\n",
    "STATIC_TENSOR_DIR = \"static_game_tensors\"\n",
    "\n",
    "# Embedding dimensions\n",
    "E_player = 18  # PlayerID embedding size\n",
    "E_position = 4\n",
    "E_team = 8\n",
    "E_season = 4\n",
    "\n",
    "# Max ranges for embedding indices\n",
    "max_normalized_player_id = 2500  # Using normalized PlayerIDs\n",
    "max_team_id = 32  # Total teams (incl. Supersonics)\n",
    "max_position_id = 3  # C, F, G, None -> 0, 1, 2, 3\n",
    "max_season_id = 22  # Seasons 2003 to 2023 -> 0 to 20\n",
    "\n",
    "# Initialize embeddings\n",
    "player_embedding = nn.Embedding(max_normalized_player_id + 1, E_player)\n",
    "position_embedding = nn.Embedding(max_position_id + 1, E_position)\n",
    "team_embedding = nn.Embedding(max_team_id + 1, E_team)\n",
    "season_embedding = nn.Embedding(max_season_id + 1, E_season)\n",
    "\n",
    "# Load the player ID mapping\n",
    "player_id_mapping = pd.read_csv(\"player_id_mapping_normalized.csv\").set_index(\"Player ID\")[\"Normalized ID\"].to_dict()\n",
    "\n",
    "def load_static_tensor(game_id, team_type, season):\n",
    "    \"\"\"\n",
    "    Load the static tensor for a team (home/away) in a specific game.\n",
    "    \"\"\"\n",
    "    tensor_path = os.path.join(STATIC_TENSOR_DIR, f\"season_{season}\", f\"{game_id}_{team_type}_static.pt\")\n",
    "    if not os.path.exists(tensor_path):\n",
    "        raise FileNotFoundError(f\"Static tensor not found: {tensor_path}\")\n",
    "    return torch.load(tensor_path)\n",
    "\n",
    "def create_player_token(player_id, position_id, team_id_for, team_id_against, season_id, static_tensor):\n",
    "    \"\"\"\n",
    "    Concatenate dynamic embeddings with static features to create a full player token.\n",
    "    \"\"\"\n",
    "    # Replace bloated player ID with normalized ID\n",
    "    normalized_player_id = player_id_mapping.get(player_id, 0)  # Default to 0 for invalid IDs\n",
    "\n",
    "    # Generate dynamic embeddings\n",
    "    player_id_tensor = player_embedding(torch.tensor(normalized_player_id, dtype=torch.long))\n",
    "    position_tensor = position_embedding(torch.tensor(position_id, dtype=torch.long))\n",
    "    team_for_tensor = team_embedding(torch.tensor(team_id_for, dtype=torch.long))\n",
    "    team_against_tensor = team_embedding(torch.tensor(team_id_against, dtype=torch.long))\n",
    "    season_tensor = season_embedding(torch.tensor(season_id, dtype=torch.long))\n",
    "    \n",
    "    # Concatenate dynamic and static features\n",
    "    return torch.cat([player_id_tensor, position_tensor, team_for_tensor, team_against_tensor, season_tensor, static_tensor])\n",
    "\n",
    "def create_game_tokens(game_id, season, home_team_id, away_team_id, games_df):\n",
    "    \"\"\"\n",
    "    Generate tokens for all players in a game (home and away).\n",
    "    \"\"\"\n",
    "    # Mapping for position to IDs\n",
    "    position_mapping = {\"C\": 0, \"F\": 1, \"G\": 2, None: 3}\n",
    "\n",
    "    # Load static tensors\n",
    "    home_static_tensor = load_static_tensor(game_id, \"home\", season)\n",
    "    away_static_tensor = load_static_tensor(game_id, \"away\", season)\n",
    "    \n",
    "    # Extract player data dynamically from the dataframe\n",
    "    home_players = [\n",
    "        {\"player_id\": int(games_df[f\"home_player_{i}_id\"]) if not pd.isna(games_df[f\"home_player_{i}_id\"]) else None,\n",
    "         \"position_id\": position_mapping.get(games_df[f\"home_player_{i}_position\"], 3)}\n",
    "        for i in range(1, 13)\n",
    "    ]\n",
    "    away_players = [\n",
    "        {\"player_id\": int(games_df[f\"away_player_{i}_id\"]) if not pd.isna(games_df[f\"away_player_{i}_id\"]) else None,\n",
    "         \"position_id\": position_mapping.get(games_df[f\"away_player_{i}_position\"], 3)}\n",
    "        for i in range(1, 13)\n",
    "    ]\n",
    "    \n",
    "    # Create player tokens for home team\n",
    "    home_tokens = []\n",
    "    for i, player in enumerate(home_players):\n",
    "        if player[\"player_id\"] is None:\n",
    "            continue\n",
    "        player_token = create_player_token(\n",
    "            player_id=player[\"player_id\"],\n",
    "            position_id=player[\"position_id\"],\n",
    "            team_id_for=home_team_id,\n",
    "            team_id_against=away_team_id,\n",
    "            season_id=season - 2003,  # Normalize season to 0-20\n",
    "            static_tensor=home_static_tensor[i]\n",
    "        )\n",
    "        home_tokens.append(player_token)\n",
    "    \n",
    "    # Create player tokens for away team\n",
    "    away_tokens = []\n",
    "    for i, player in enumerate(away_players):\n",
    "        if player[\"player_id\"] is None:\n",
    "            continue\n",
    "        player_token = create_player_token(\n",
    "            player_id=player[\"player_id\"],\n",
    "            position_id=player[\"position_id\"],\n",
    "            team_id_for=away_team_id,\n",
    "            team_id_against=home_team_id,\n",
    "            season_id=season - 2003,  # Normalize season to 0-20\n",
    "            static_tensor=away_static_tensor[i]\n",
    "        )\n",
    "        away_tokens.append(player_token)\n",
    "    \n",
    "    # Ensure each team has exactly 12 tokens\n",
    "    while len(home_tokens) < 12:\n",
    "        home_tokens.append(torch.zeros_like(home_tokens[0]))\n",
    "    while len(away_tokens) < 12:\n",
    "        away_tokens.append(torch.zeros_like(away_tokens[0]))\n",
    "    \n",
    "    # Stack tokens for both teams\n",
    "    home_tokens = torch.stack(home_tokens)\n",
    "    away_tokens = torch.stack(away_tokens)\n",
    "    return home_tokens, away_tokens\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "game_id = 15486\n",
    "season = 2003\n",
    "home_team_id = 13  # Lakers\n",
    "away_team_id = 7  # Mavericks\n",
    "\n",
    "# Load games dataframe\n",
    "games_df = pd.read_csv(\"expanded_games_2003_2023.csv\").set_index(\"game_id\").loc[game_id]\n",
    "\n",
    "# Generate tokens\n",
    "home_tokens, away_tokens = create_game_tokens(game_id, season, home_team_id, away_team_id, games_df)\n",
    "\n",
    "# Print results\n",
    "print(f\"Home Tokens Shape: {home_tokens.shape}\")\n",
    "print(f\"Away Tokens Shape: {away_tokens.shape}\")\n",
    "for i in range(12):\n",
    "    print(f\"Home Token {i}:\\n{home_tokens[i]}\")\n",
    "for i in range(12):\n",
    "    print(f\"Away Token {i}:\\n{away_tokens[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomy of our 24x48 Tensor Inputs\n",
    "\n",
    "For each NBA game, we represent the **home team** and **away team** with two separate tensors, each with a shape of `12x48`. These tensors include both **static features** and **dynamic embeddings** for the top 12 players from each team. Here's a breakdown of the tensor structure:\n",
    "\n",
    "---\n",
    "\n",
    "#### **Per Team Tensor Dimensions**\n",
    "- **12 Rows**: Each row represents a player token, ordered by minutes played during the game.\n",
    "- **48 Columns**: Each column corresponds to a feature derived from either static or dynamic data.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Feature Breakdown (48 Features per Player)**\n",
    "\n",
    "1. **Dynamic Embeddings (42 Features)**:\n",
    "   - **PlayerID Embedding (18 Features)**:\n",
    "     - A learned embedding representing the unique player.\n",
    "   - **PositionID Embedding (4 Features)**:\n",
    "     - A learned embedding representing the player's position (`C`, `F`, `G`, or `None`).\n",
    "   - **TeamID Embedding (16 Features)**:\n",
    "     - Two separate embeddings for:\n",
    "       1. The team the player is playing **for** (8 features).\n",
    "       2. The team the player is playing **against** (8 features).\n",
    "   - **Season Embedding (4 Features)**:\n",
    "     - A learned embedding representing the season the game took place in.\n",
    "\n",
    "2. **Static Features (6 Features)**:\n",
    "   - **Minutes Played** (normalized): The player's total minutes on the court.\n",
    "   - **Offensive Rating** (normalized): Player's offensive efficiency.\n",
    "   - **Defensive Rating** (normalized): Player's defensive efficiency.\n",
    "   - **Usage Percentage** (normalized): Player's involvement in the offense.\n",
    "   - **Height** (normalized): Player's height in inches.\n",
    "   - **Weight** (normalized): Player's weight in pounds.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Token Example**\n",
    "For a single game:\n",
    "- **Shape**: `[1, 48]`\n",
    "- **Example for Player 1**:\n",
    "  ```plaintext\n",
    "  tensor([PlayerID Embedding (18), \n",
    "          PositionID Embedding (4), \n",
    "          Team For Embedding (4), \n",
    "          Team Against Embedding (4), \n",
    "          Season Embedding (4), \n",
    "          Minutes Played (1), \n",
    "          Offensive Rating (1), \n",
    "          Defensive Rating (1), \n",
    "          Usage Percentage (1), \n",
    "          Height (1), \n",
    "          Weight (1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping game 17959: Static tensor not found: static_game_tensors/season_2003/17959_home_static.pt\n",
      "Epoch 1/10, Loss: 116693.6116\n",
      "Skipping game 17959: Static tensor not found: static_game_tensors/season_2003/17959_home_static.pt\n",
      "Epoch 2/10, Loss: 12133.0582\n",
      "Skipping game 17959: Static tensor not found: static_game_tensors/season_2003/17959_home_static.pt\n",
      "Epoch 3/10, Loss: 11950.9500\n",
      "Skipping game 17959: Static tensor not found: static_game_tensors/season_2003/17959_home_static.pt\n",
      "Epoch 4/10, Loss: 12181.9934\n",
      "Skipping game 17959: Static tensor not found: static_game_tensors/season_2003/17959_home_static.pt\n",
      "Epoch 5/10, Loss: 12274.4950\n",
      "Skipping game 17959: Static tensor not found: static_game_tensors/season_2003/17959_home_static.pt\n",
      "Epoch 6/10, Loss: 11958.3569\n",
      "Skipping game 17959: Static tensor not found: static_game_tensors/season_2003/17959_home_static.pt\n",
      "Epoch 7/10, Loss: 11522.0214\n",
      "Skipping game 17959: Static tensor not found: static_game_tensors/season_2003/17959_home_static.pt\n",
      "Epoch 8/10, Loss: 11261.8006\n",
      "Skipping game 17959: Static tensor not found: static_game_tensors/season_2003/17959_home_static.pt\n",
      "Epoch 9/10, Loss: 11527.2286\n",
      "Skipping game 17959: Static tensor not found: static_game_tensors/season_2003/17959_home_static.pt\n",
      "Epoch 10/10, Loss: 10686.1041\n"
     ]
    }
   ],
   "source": [
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=(\n",
    "        \"enable_nested_tensor is True, but self.use_nested_tensor is False because\"\n",
    "        \" encoder_layer.self_attn.batch_first was not True\"\n",
    "    ),\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\"You are using `torch.load` with `weights_only=False`.*\",\n",
    "    category=FutureWarning\n",
    ")\n",
    "\n",
    "STATIC_TENSOR_DIR = \"static_game_tensors\"\n",
    "\n",
    "# Dataset class for the 2003 season\n",
    "class NBA2003Dataset(Dataset):\n",
    "    def __init__(self, games_df):\n",
    "        self.games_df = games_df[games_df[\"season\"] == 2003]  # Filter for 2003 season\n",
    "        self.game_ids = self.games_df.index.tolist()\n",
    "        self.position_mapping = {\"C\": 0, \"F\": 1, \"G\": 2, None: 3}  # Map positions to integers\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.game_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        game_id = self.game_ids[idx]\n",
    "\n",
    "        try:\n",
    "            # Extract the row as a Series\n",
    "            game_row = self.games_df.loc[game_id]\n",
    "            if isinstance(game_row, pd.DataFrame):  # Handle duplicate game_id scenario\n",
    "                game_row = game_row.iloc[0]  # Take the first occurrence\n",
    "\n",
    "            # Ensure all scalar values\n",
    "            season = int(game_row[\"season\"])\n",
    "            home_team_id = int(game_row[\"home_team_id\"])\n",
    "            away_team_id = int(game_row[\"visitor_team_id\"])\n",
    "\n",
    "            # Load static tensors\n",
    "            home_static_tensor = load_static_tensor(game_id, \"home\", season)\n",
    "            away_static_tensor = load_static_tensor(game_id, \"away\", season)\n",
    "\n",
    "            # Extract player data dynamically from the dataframe\n",
    "            home_players = [\n",
    "                {\"player_id\": int(game_row[f\"home_player_{i}_id\"]) if not pd.isna(game_row[f\"home_player_{i}_id\"]) else None,\n",
    "                 \"position_id\": self.position_mapping.get(game_row[f\"home_player_{i}_position\"], 3)}\n",
    "                for i in range(1, 13)\n",
    "            ]\n",
    "            away_players = [\n",
    "                {\"player_id\": int(game_row[f\"away_player_{i}_id\"]) if not pd.isna(game_row[f\"away_player_{i}_id\"]) else None,\n",
    "                 \"position_id\": self.position_mapping.get(game_row[f\"away_player_{i}_position\"], 3)}\n",
    "                for i in range(1, 13)\n",
    "            ]\n",
    "\n",
    "            # Create tokens for both teams\n",
    "            home_tokens, away_tokens = create_game_tokens(\n",
    "                game_id, season, home_team_id, away_team_id, game_row\n",
    "            )\n",
    "\n",
    "            # Target extraction\n",
    "            target = torch.tensor([game_row[\"home_team_score\"], game_row[\"visitor_team_score\"]], dtype=torch.float32)\n",
    "\n",
    "            return home_tokens, away_tokens, target\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Skipping game {game_id}: {e}\")\n",
    "            return None  # Returning None will be handled in the DataLoader collate function\n",
    "\n",
    "\n",
    "# Custom collate function to handle None\n",
    "def custom_collate_fn(batch):\n",
    "    # Filter out None values\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None, None, None\n",
    "    return default_collate(batch)  # Use PyTorch's default collate for valid data\n",
    "\n",
    "\n",
    "# Initialize dataset and data loader\n",
    "games_df = pd.read_csv(\"expanded_games_2003_2023.csv\").set_index(\"game_id\")\n",
    "dataset = NBA2003Dataset(games_df)\n",
    "data_loader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=0, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Model definition\n",
    "class LineupLab(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads, num_layers, output_dim=2):\n",
    "        super(LineupLab, self).__init__()\n",
    "        self.home_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim),\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.away_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim),\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.combined_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim),\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),  # Flatten the tokens and feature dimensions for each batch\n",
    "            nn.Linear(input_dim * 24, 128),  # Adjust for 24 tokens (12 home + 12 away)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, home_tokens, away_tokens):\n",
    "        # Apply home and away transformers\n",
    "        home_output = self.home_transformer(home_tokens)  # Shape: [batch_size, 12, input_dim]\n",
    "        away_output = self.away_transformer(away_tokens)  # Shape: [batch_size, 12, input_dim]\n",
    "        \n",
    "        # Concatenate outputs along the token dimension\n",
    "        combined_input = torch.cat((home_output, away_output), dim=1)  # Shape: [batch_size, 24, input_dim]\n",
    "        \n",
    "        # Pass through combined transformer\n",
    "        combined_output = self.combined_transformer(combined_input)  # Shape: [batch_size, 24, input_dim]\n",
    "        \n",
    "        # Flatten and pass through fully connected layers\n",
    "        scores = self.fc(combined_output)  # Shape: [batch_size, output_dim]\n",
    "        return scores\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "input_dim = 48\n",
    "hidden_dim = 128\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "model = LineupLab(input_dim, hidden_dim, num_heads, num_layers)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in data_loader:\n",
    "        if batch is None:\n",
    "            continue\n",
    "        home_tokens, away_tokens, target = batch\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(home_tokens, away_tokens)\n",
    "        loss = loss_fn(prediction, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game ID: 15882412\n",
      "Season: 2023\n",
      "Predicted Scores: Home: 117.67, Away: 108.93\n",
      "Actual Scores: Home: 112.0, Away: 89.0\n"
     ]
    }
   ],
   "source": [
    "# Function to test the model on a single game\n",
    "def test_model_on_game(game_id, season, model, games_df, static_tensor_dir=\"static_game_tensors\"):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    position_mapping = {\"C\": 0, \"F\": 1, \"G\": 2, None: 3}  # Map positions to integers\n",
    "\n",
    "    try:\n",
    "        # Extract game row from DataFrame\n",
    "        game_row = games_df.loc[game_id]\n",
    "        if isinstance(game_row, pd.DataFrame):  # Handle duplicate game_id scenario\n",
    "            game_row = game_row.iloc[0]  # Take the first occurrence\n",
    "\n",
    "        # Load static tensors\n",
    "        home_static_tensor = load_static_tensor(game_id, \"home\", season)\n",
    "        away_static_tensor = load_static_tensor(game_id, \"away\", season)\n",
    "\n",
    "        # Extract player data dynamically\n",
    "        home_players = [\n",
    "            {\"player_id\": int(game_row[f\"home_player_{i}_id\"]) if not pd.isna(game_row[f\"home_player_{i}_id\"]) else None,\n",
    "             \"position_id\": position_mapping.get(game_row[f\"home_player_{i}_position\"], 3)}\n",
    "            for i in range(1, 13)\n",
    "        ]\n",
    "        away_players = [\n",
    "            {\"player_id\": int(game_row[f\"away_player_{i}_id\"]) if not pd.isna(game_row[f\"away_player_{i}_id\"]) else None,\n",
    "             \"position_id\": position_mapping.get(game_row[f\"away_player_{i}_position\"], 3)}\n",
    "            for i in range(1, 13)\n",
    "        ]\n",
    "\n",
    "        # Create tokens for both teams\n",
    "        home_tokens, away_tokens = create_game_tokens(\n",
    "            game_id, season, int(game_row[\"home_team_id\"]), int(game_row[\"visitor_team_id\"]), game_row\n",
    "        )\n",
    "\n",
    "        # Check if home_tokens and away_tokens are already tensors, otherwise convert them\n",
    "        if not isinstance(home_tokens, torch.Tensor):\n",
    "            home_tokens_tensor = torch.tensor(home_tokens).unsqueeze(0)  # Shape: [1, 12, input_dim]\n",
    "        else:\n",
    "            home_tokens_tensor = home_tokens.unsqueeze(0)\n",
    "\n",
    "        if not isinstance(away_tokens, torch.Tensor):\n",
    "            away_tokens_tensor = torch.tensor(away_tokens).unsqueeze(0)  # Shape: [1, 12, input_dim]\n",
    "        else:\n",
    "            away_tokens_tensor = away_tokens.unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "        # Perform prediction\n",
    "        with torch.no_grad():\n",
    "            predicted_scores = model(home_tokens_tensor, away_tokens_tensor)\n",
    "\n",
    "        # Extract ground truth scores\n",
    "        actual_scores = torch.tensor(\n",
    "            [game_row[\"home_team_score\"], game_row[\"visitor_team_score\"]], dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        # Output results\n",
    "        print(f\"Game ID: {game_id}\")\n",
    "        print(f\"Season: {season}\")\n",
    "        print(f\"Predicted Scores: Home: {predicted_scores[0, 0].item():.2f}, Away: {predicted_scores[0, 1].item():.2f}\")\n",
    "        print(f\"Actual Scores: Home: {actual_scores[0].item()}, Away: {actual_scores[1].item()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing game {game_id}: {e}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "games_df = pd.read_csv(\"expanded_games_2003_2023.csv\").set_index(\"game_id\")\n",
    "game_id_to_test = 15882412  # Replace with the actual game ID from another season\n",
    "season_to_test = 2023    # Replace with the actual season for the game ID\n",
    "\n",
    "test_model_on_game(game_id_to_test, season_to_test, model, games_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 20685 games\n",
      "Validation data: 3649 games\n",
      "Testing data: 2613 games\n",
      "Skipping game 29006: Static tensor not found: static_game_tensors/season_2013/29006_home_static.pt\n",
      "Skipping game 34983: Static tensor not found: static_game_tensors/season_2016/34983_home_static.pt\n",
      "Skipping game 22715: Static tensor not found: static_game_tensors/season_2007/22715_home_static.pt\n",
      "Skipping game 49061: Static tensor not found: static_game_tensors/season_2018/49061_home_static.pt\n",
      "Skipping game 49071: Static tensor not found: static_game_tensors/season_2018/49071_home_static.pt\n",
      "Skipping game 19630: Static tensor not found: static_game_tensors/season_2005/19630_home_static.pt\n",
      "Skipping game 48787: Static tensor not found: static_game_tensors/season_2018/48787_home_static.pt\n",
      "Skipping game 19068: Static tensor not found: static_game_tensors/season_2006/19068_home_static.pt\n",
      "Skipping game 49149: Static tensor not found: static_game_tensors/season_2018/49149_home_static.pt\n",
      "Skipping game 17959: Static tensor not found: static_game_tensors/season_2003/17959_home_static.pt\n",
      "Skipping game 21576: Static tensor not found: static_game_tensors/season_2008/21576_home_static.pt\n",
      "Skipping game 31027: Static tensor not found: static_game_tensors/season_2013/31027_home_static.pt\n",
      "Skipping game 46970: Static tensor not found: static_game_tensors/season_2018/46970_home_static.pt\n",
      "Skipping game 25294: Static tensor not found: static_game_tensors/season_2009/25294_home_static.pt\n",
      "Epoch 1/10, Train Loss: 340995.6456, Val Loss: 54938.4941\n",
      "Train Loss Per Team Per Game: 8.2426, Val Loss Per Team Per Game: 7.5279\n",
      "Skipping game 22715: Static tensor not found: static_game_tensors/season_2007/22715_home_static.pt\n",
      "Skipping game 49071: Static tensor not found: static_game_tensors/season_2018/49071_home_static.pt\n",
      "Skipping game 31027: Static tensor not found: static_game_tensors/season_2013/31027_home_static.pt\n",
      "Skipping game 21576: Static tensor not found: static_game_tensors/season_2008/21576_home_static.pt\n",
      "Skipping game 29006: Static tensor not found: static_game_tensors/season_2013/29006_home_static.pt\n",
      "Skipping game 49149: Static tensor not found: static_game_tensors/season_2018/49149_home_static.pt\n",
      "Skipping game 19068: Static tensor not found: static_game_tensors/season_2006/19068_home_static.pt\n",
      "Skipping game 46970: Static tensor not found: static_game_tensors/season_2018/46970_home_static.pt\n",
      "Skipping game 19630: Static tensor not found: static_game_tensors/season_2005/19630_home_static.pt\n",
      "Skipping game 25294: Static tensor not found: static_game_tensors/season_2009/25294_home_static.pt\n",
      "Skipping game 34983: Static tensor not found: static_game_tensors/season_2016/34983_home_static.pt\n",
      "Skipping game 49061: Static tensor not found: static_game_tensors/season_2018/49061_home_static.pt\n",
      "Skipping game 17959: Static tensor not found: static_game_tensors/season_2003/17959_home_static.pt\n",
      "Skipping game 48787: Static tensor not found: static_game_tensors/season_2018/48787_home_static.pt\n",
      "Epoch 2/10, Train Loss: 190034.1544, Val Loss: 55333.2316\n",
      "Train Loss Per Team Per Game: 4.5935, Val Loss Per Team Per Game: 7.5820\n",
      "Skipping game 29006: Static tensor not found: static_game_tensors/season_2013/29006_home_static.pt\n",
      "Skipping game 34983: Static tensor not found: static_game_tensors/season_2016/34983_home_static.pt\n",
      "Skipping game 25294: Static tensor not found: static_game_tensors/season_2009/25294_home_static.pt\n",
      "Skipping game 21576: Static tensor not found: static_game_tensors/season_2008/21576_home_static.pt\n",
      "Skipping game 46970: Static tensor not found: static_game_tensors/season_2018/46970_home_static.pt\n",
      "Skipping game 48787: Static tensor not found: static_game_tensors/season_2018/48787_home_static.pt\n",
      "Skipping game 31027: Static tensor not found: static_game_tensors/season_2013/31027_home_static.pt\n",
      "Skipping game 49061: Static tensor not found: static_game_tensors/season_2018/49061_home_static.pt\n",
      "Skipping game 49149: Static tensor not found: static_game_tensors/season_2018/49149_home_static.pt\n",
      "Skipping game 19068: Static tensor not found: static_game_tensors/season_2006/19068_home_static.pt\n",
      "Skipping game 19630: Static tensor not found: static_game_tensors/season_2005/19630_home_static.pt\n",
      "Skipping game 22715: Static tensor not found: static_game_tensors/season_2007/22715_home_static.pt\n",
      "Skipping game 17959: Static tensor not found: static_game_tensors/season_2003/17959_home_static.pt\n",
      "Skipping game 49071: Static tensor not found: static_game_tensors/season_2018/49071_home_static.pt\n",
      "Epoch 3/10, Train Loss: 182597.1721, Val Loss: 40417.5109\n",
      "Train Loss Per Team Per Game: 4.4138, Val Loss Per Team Per Game: 5.5382\n",
      "Skipping game 48787: Static tensor not found: static_game_tensors/season_2018/48787_home_static.pt\n",
      "Skipping game 25294: Static tensor not found: static_game_tensors/season_2009/25294_home_static.pt\n",
      "Skipping game 34983: Static tensor not found: static_game_tensors/season_2016/34983_home_static.pt\n",
      "Skipping game 22715: Static tensor not found: static_game_tensors/season_2007/22715_home_static.pt\n",
      "Skipping game 49071: Static tensor not found: static_game_tensors/season_2018/49071_home_static.pt\n",
      "Skipping game 46970: Static tensor not found: static_game_tensors/season_2018/46970_home_static.pt\n",
      "Skipping game 31027: Static tensor not found: static_game_tensors/season_2013/31027_home_static.pt\n",
      "Skipping game 29006: Static tensor not found: static_game_tensors/season_2013/29006_home_static.pt\n",
      "Skipping game 49149: Static tensor not found: static_game_tensors/season_2018/49149_home_static.pt\n",
      "Skipping game 17959: Static tensor not found: static_game_tensors/season_2003/17959_home_static.pt\n",
      "Skipping game 19630: Static tensor not found: static_game_tensors/season_2005/19630_home_static.pt\n",
      "Skipping game 21576: Static tensor not found: static_game_tensors/season_2008/21576_home_static.pt\n",
      "Skipping game 49061: Static tensor not found: static_game_tensors/season_2018/49061_home_static.pt\n",
      "Skipping game 19068: Static tensor not found: static_game_tensors/season_2006/19068_home_static.pt\n",
      "Epoch 4/10, Train Loss: 180152.7368, Val Loss: 44040.1407\n",
      "Train Loss Per Team Per Game: 4.3547, Val Loss Per Team Per Game: 6.0345\n",
      "Skipping game 46970: Static tensor not found: static_game_tensors/season_2018/46970_home_static.pt\n",
      "Skipping game 49061: Static tensor not found: static_game_tensors/season_2018/49061_home_static.pt\n",
      "Skipping game 25294: Static tensor not found: static_game_tensors/season_2009/25294_home_static.pt\n",
      "Skipping game 31027: Static tensor not found: static_game_tensors/season_2013/31027_home_static.pt\n",
      "Skipping game 19068: Static tensor not found: static_game_tensors/season_2006/19068_home_static.pt\n",
      "Skipping game 29006: Static tensor not found: static_game_tensors/season_2013/29006_home_static.pt\n",
      "Skipping game 48787: Static tensor not found: static_game_tensors/season_2018/48787_home_static.pt\n",
      "Skipping game 19630: Static tensor not found: static_game_tensors/season_2005/19630_home_static.pt\n",
      "Skipping game 49071: Static tensor not found: static_game_tensors/season_2018/49071_home_static.pt\n",
      "Skipping game 49149: Static tensor not found: static_game_tensors/season_2018/49149_home_static.pt\n",
      "Skipping game 22715: Static tensor not found: static_game_tensors/season_2007/22715_home_static.pt\n",
      "Skipping game 34983: Static tensor not found: static_game_tensors/season_2016/34983_home_static.pt\n",
      "Skipping game 21576: Static tensor not found: static_game_tensors/season_2008/21576_home_static.pt\n",
      "Skipping game 17959: Static tensor not found: static_game_tensors/season_2003/17959_home_static.pt\n",
      "Epoch 5/10, Train Loss: 173841.6669, Val Loss: 40466.4022\n",
      "Train Loss Per Team Per Game: 4.2021, Val Loss Per Team Per Game: 5.5449\n",
      "Skipping game 17959: Static tensor not found: static_game_tensors/season_2003/17959_home_static.pt\n",
      "Skipping game 49061: Static tensor not found: static_game_tensors/season_2018/49061_home_static.pt\n",
      "Skipping game 22715: Static tensor not found: static_game_tensors/season_2007/22715_home_static.pt\n",
      "Skipping game 34983: Static tensor not found: static_game_tensors/season_2016/34983_home_static.pt\n",
      "Skipping game 19630: Static tensor not found: static_game_tensors/season_2005/19630_home_static.pt\n",
      "Skipping game 25294: Static tensor not found: static_game_tensors/season_2009/25294_home_static.pt\n",
      "Skipping game 49071: Static tensor not found: static_game_tensors/season_2018/49071_home_static.pt\n",
      "Skipping game 19068: Static tensor not found: static_game_tensors/season_2006/19068_home_static.pt\n",
      "Skipping game 21576: Static tensor not found: static_game_tensors/season_2008/21576_home_static.pt\n",
      "Skipping game 49149: Static tensor not found: static_game_tensors/season_2018/49149_home_static.pt\n",
      "Skipping game 46970: Static tensor not found: static_game_tensors/season_2018/46970_home_static.pt\n",
      "Skipping game 31027: Static tensor not found: static_game_tensors/season_2013/31027_home_static.pt\n",
      "Skipping game 29006: Static tensor not found: static_game_tensors/season_2013/29006_home_static.pt\n",
      "Skipping game 48787: Static tensor not found: static_game_tensors/season_2018/48787_home_static.pt\n",
      "Epoch 6/10, Train Loss: 167787.0322, Val Loss: 57410.8204\n",
      "Train Loss Per Team Per Game: 4.0558, Val Loss Per Team Per Game: 7.8667\n",
      "Skipping game 49061: Static tensor not found: static_game_tensors/season_2018/49061_home_static.pt\n",
      "Skipping game 19630: Static tensor not found: static_game_tensors/season_2005/19630_home_static.pt\n",
      "Skipping game 22715: Static tensor not found: static_game_tensors/season_2007/22715_home_static.pt\n",
      "Skipping game 49071: Static tensor not found: static_game_tensors/season_2018/49071_home_static.pt\n",
      "Skipping game 21576: Static tensor not found: static_game_tensors/season_2008/21576_home_static.pt\n",
      "Skipping game 17959: Static tensor not found: static_game_tensors/season_2003/17959_home_static.pt\n",
      "Skipping game 48787: Static tensor not found: static_game_tensors/season_2018/48787_home_static.pt\n",
      "Skipping game 29006: Static tensor not found: static_game_tensors/season_2013/29006_home_static.pt\n",
      "Skipping game 34983: Static tensor not found: static_game_tensors/season_2016/34983_home_static.pt\n",
      "Skipping game 19068: Static tensor not found: static_game_tensors/season_2006/19068_home_static.pt\n",
      "Skipping game 31027: Static tensor not found: static_game_tensors/season_2013/31027_home_static.pt\n",
      "Skipping game 25294: Static tensor not found: static_game_tensors/season_2009/25294_home_static.pt\n",
      "Skipping game 46970: Static tensor not found: static_game_tensors/season_2018/46970_home_static.pt\n",
      "Skipping game 49149: Static tensor not found: static_game_tensors/season_2018/49149_home_static.pt\n",
      "Epoch 7/10, Train Loss: 161263.4928, Val Loss: 41395.4298\n",
      "Train Loss Per Team Per Game: 3.8981, Val Loss Per Team Per Game: 5.6722\n",
      "Skipping game 19630: Static tensor not found: static_game_tensors/season_2005/19630_home_static.pt\n",
      "Skipping game 46970: Static tensor not found: static_game_tensors/season_2018/46970_home_static.pt\n",
      "Skipping game 48787: Static tensor not found: static_game_tensors/season_2018/48787_home_static.pt\n",
      "Skipping game 49071: Static tensor not found: static_game_tensors/season_2018/49071_home_static.pt\n",
      "Skipping game 29006: Static tensor not found: static_game_tensors/season_2013/29006_home_static.pt\n",
      "Skipping game 25294: Static tensor not found: static_game_tensors/season_2009/25294_home_static.pt\n",
      "Skipping game 22715: Static tensor not found: static_game_tensors/season_2007/22715_home_static.pt\n",
      "Skipping game 49149: Static tensor not found: static_game_tensors/season_2018/49149_home_static.pt\n",
      "Skipping game 34983: Static tensor not found: static_game_tensors/season_2016/34983_home_static.pt\n",
      "Skipping game 19068: Static tensor not found: static_game_tensors/season_2006/19068_home_static.pt\n",
      "Skipping game 17959: Static tensor not found: static_game_tensors/season_2003/17959_home_static.pt\n",
      "Skipping game 31027: Static tensor not found: static_game_tensors/season_2013/31027_home_static.pt\n",
      "Skipping game 21576: Static tensor not found: static_game_tensors/season_2008/21576_home_static.pt\n",
      "Skipping game 49061: Static tensor not found: static_game_tensors/season_2018/49061_home_static.pt\n",
      "Epoch 8/10, Train Loss: 156091.7543, Val Loss: 52456.5951\n",
      "Train Loss Per Team Per Game: 3.7731, Val Loss Per Team Per Game: 7.1878\n",
      "Skipping game 49071: Static tensor not found: static_game_tensors/season_2018/49071_home_static.pt\n",
      "Skipping game 49149: Static tensor not found: static_game_tensors/season_2018/49149_home_static.pt\n",
      "Skipping game 31027: Static tensor not found: static_game_tensors/season_2013/31027_home_static.pt\n",
      "Skipping game 48787: Static tensor not found: static_game_tensors/season_2018/48787_home_static.pt\n",
      "Skipping game 17959: Static tensor not found: static_game_tensors/season_2003/17959_home_static.pt\n",
      "Skipping game 19630: Static tensor not found: static_game_tensors/season_2005/19630_home_static.pt\n",
      "Skipping game 46970: Static tensor not found: static_game_tensors/season_2018/46970_home_static.pt\n",
      "Skipping game 21576: Static tensor not found: static_game_tensors/season_2008/21576_home_static.pt\n",
      "Skipping game 49061: Static tensor not found: static_game_tensors/season_2018/49061_home_static.pt\n",
      "Skipping game 34983: Static tensor not found: static_game_tensors/season_2016/34983_home_static.pt\n",
      "Skipping game 29006: Static tensor not found: static_game_tensors/season_2013/29006_home_static.pt\n",
      "Skipping game 19068: Static tensor not found: static_game_tensors/season_2006/19068_home_static.pt\n",
      "Skipping game 22715: Static tensor not found: static_game_tensors/season_2007/22715_home_static.pt\n",
      "Skipping game 25294: Static tensor not found: static_game_tensors/season_2009/25294_home_static.pt\n",
      "Epoch 9/10, Train Loss: 152354.7971, Val Loss: 55449.7397\n",
      "Train Loss Per Team Per Game: 3.6827, Val Loss Per Team Per Game: 7.5979\n",
      "Skipping game 25294: Static tensor not found: static_game_tensors/season_2009/25294_home_static.pt\n",
      "Skipping game 22715: Static tensor not found: static_game_tensors/season_2007/22715_home_static.pt\n",
      "Skipping game 21576: Static tensor not found: static_game_tensors/season_2008/21576_home_static.pt\n",
      "Skipping game 17959: Static tensor not found: static_game_tensors/season_2003/17959_home_static.pt\n",
      "Skipping game 48787: Static tensor not found: static_game_tensors/season_2018/48787_home_static.pt\n",
      "Skipping game 19630: Static tensor not found: static_game_tensors/season_2005/19630_home_static.pt\n",
      "Skipping game 19068: Static tensor not found: static_game_tensors/season_2006/19068_home_static.pt\n",
      "Skipping game 49071: Static tensor not found: static_game_tensors/season_2018/49071_home_static.pt\n",
      "Skipping game 46970: Static tensor not found: static_game_tensors/season_2018/46970_home_static.pt\n",
      "Skipping game 49149: Static tensor not found: static_game_tensors/season_2018/49149_home_static.pt\n",
      "Skipping game 31027: Static tensor not found: static_game_tensors/season_2013/31027_home_static.pt\n",
      "Skipping game 29006: Static tensor not found: static_game_tensors/season_2013/29006_home_static.pt\n",
      "Skipping game 49061: Static tensor not found: static_game_tensors/season_2018/49061_home_static.pt\n",
      "Skipping game 34983: Static tensor not found: static_game_tensors/season_2016/34983_home_static.pt\n",
      "Epoch 10/10, Train Loss: 149761.2204, Val Loss: 49690.2187\n",
      "Train Loss Per Team Per Game: 3.6200, Val Loss Per Team Per Game: 6.8087\n",
      "Cumulative Test Loss: 31041.2145\n",
      "Test Loss Per Game Per Team: 5.9398\n"
     ]
    }
   ],
   "source": [
    "# Define year ranges for splits\n",
    "train_years = range(2003, 2019)  # Training: 2003â€“2018\n",
    "val_years = range(2019, 2022)    # Validation: 2019â€“2021\n",
    "test_years = range(2022, 2024)   # Testing: 2022â€“2023\n",
    "\n",
    "# Split the data by year\n",
    "games_df = pd.read_csv(\"expanded_games_2003_2023.csv\").set_index(\"game_id\")\n",
    "train_df = games_df[games_df[\"season\"].isin(train_years)]\n",
    "val_df = games_df[games_df[\"season\"].isin(val_years)]\n",
    "test_df = games_df[games_df[\"season\"].isin(test_years)]\n",
    "\n",
    "# Print sizes to verify splits\n",
    "print(f\"Training data: {len(train_df)} games\")\n",
    "print(f\"Validation data: {len(val_df)} games\")\n",
    "print(f\"Testing data: {len(test_df)} games\")\n",
    "\n",
    "# Dataset class for all seasons\n",
    "class NBAFullDataset(Dataset):\n",
    "    def __init__(self, games_df):\n",
    "        self.games_df = games_df\n",
    "        self.game_ids = self.games_df.index.tolist()\n",
    "        self.position_mapping = {\"C\": 0, \"F\": 1, \"G\": 2, None: 3}  # Map positions to integers\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.game_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        game_id = self.game_ids[idx]\n",
    "        try:\n",
    "            # Extract the row as a Series\n",
    "            game_row = self.games_df.loc[game_id]\n",
    "            if isinstance(game_row, pd.DataFrame):  # Handle duplicate game_id scenario\n",
    "                game_row = game_row.iloc[0]  # Take the first occurrence\n",
    "\n",
    "            # Ensure all scalar values\n",
    "            season = int(game_row[\"season\"])\n",
    "            home_team_id = int(game_row[\"home_team_id\"])\n",
    "            away_team_id = int(game_row[\"visitor_team_id\"])\n",
    "\n",
    "            # Load static tensors\n",
    "            home_static_tensor_path = f\"static_game_tensors/season_{season}/{game_id}_home_static.pt\"\n",
    "            away_static_tensor_path = f\"static_game_tensors/season_{season}/{game_id}_away_static.pt\"\n",
    "            \n",
    "            # Skip if static tensors are missing\n",
    "            if not os.path.exists(home_static_tensor_path) or not os.path.exists(away_static_tensor_path):\n",
    "                raise FileNotFoundError(f\"Static tensor not found: {home_static_tensor_path}\")\n",
    "\n",
    "            home_static_tensor = torch.load(home_static_tensor_path)\n",
    "            away_static_tensor = torch.load(away_static_tensor_path)\n",
    "\n",
    "            # Extract player data dynamically from the dataframe\n",
    "            home_players = [\n",
    "                {\"player_id\": int(game_row[f\"home_player_{i}_id\"]) if not pd.isna(game_row[f\"home_player_{i}_id\"]) else None,\n",
    "                 \"position_id\": self.position_mapping.get(game_row[f\"home_player_{i}_position\"], 3)}\n",
    "                for i in range(1, 13)\n",
    "            ]\n",
    "            away_players = [\n",
    "                {\"player_id\": int(game_row[f\"away_player_{i}_id\"]) if not pd.isna(game_row[f\"away_player_{i}_id\"]) else None,\n",
    "                 \"position_id\": self.position_mapping.get(game_row[f\"away_player_{i}_position\"], 3)}\n",
    "                for i in range(1, 13)\n",
    "            ]\n",
    "\n",
    "            # Create tokens for both teams\n",
    "            home_tokens, away_tokens = create_game_tokens(\n",
    "                game_id, season, home_team_id, away_team_id, game_row\n",
    "            )\n",
    "\n",
    "            # Target extraction\n",
    "            target = torch.tensor([game_row[\"home_team_score\"], game_row[\"visitor_team_score\"]], dtype=torch.float32)\n",
    "\n",
    "            return home_tokens, away_tokens, target\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Skipping game {game_id}: {e}\")\n",
    "            return None  # Return None for missing data\n",
    "\n",
    "\n",
    "# Custom collate function to handle None\n",
    "def custom_collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]  # Filter out None values\n",
    "    if len(batch) == 0:\n",
    "        return None, None, None\n",
    "    return default_collate(batch)  # Use PyTorch's default collate for valid data\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NBAFullDataset(train_df)\n",
    "val_dataset = NBAFullDataset(val_df)\n",
    "test_dataset = NBAFullDataset(test_df)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Model definition\n",
    "class LineupLab(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads, num_layers, output_dim=2):\n",
    "        super(LineupLab, self).__init__()\n",
    "        self.home_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim),\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.away_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim),\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.combined_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim),\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),  # Flatten the tokens and feature dimensions for each batch\n",
    "            nn.Linear(input_dim * 24, 128),  # Adjust for 24 tokens (12 home + 12 away)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, home_tokens, away_tokens):\n",
    "        # Apply home and away transformers\n",
    "        home_output = self.home_transformer(home_tokens)  # Shape: [batch_size, 12, input_dim]\n",
    "        away_output = self.away_transformer(away_tokens)  # Shape: [batch_size, 12, input_dim]\n",
    "        \n",
    "        # Concatenate outputs along the token dimension\n",
    "        combined_input = torch.cat((home_output, away_output), dim=1)  # Shape: [batch_size, 24, input_dim]\n",
    "        \n",
    "        # Pass through combined transformer\n",
    "        combined_output = self.combined_transformer(combined_input)  # Shape: [batch_size, 24, input_dim]\n",
    "        \n",
    "        # Flatten and pass through fully connected layers\n",
    "        scores = self.fc(combined_output)  # Shape: [batch_size, output_dim]\n",
    "        return scores\n",
    "\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "input_dim = 48\n",
    "hidden_dim = 128\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "model = LineupLab(input_dim, hidden_dim, num_heads, num_layers)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training and validation loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        if batch is None:\n",
    "            continue\n",
    "        home_tokens, away_tokens, target = batch\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(home_tokens, away_tokens)\n",
    "        loss = loss_fn(prediction, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            if batch is None:\n",
    "                continue\n",
    "            home_tokens, away_tokens, target = batch\n",
    "            prediction = model(home_tokens, away_tokens)\n",
    "            loss = loss_fn(prediction, target)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train Loss Per Team Per Game: {train_loss / (2 * len(train_loader.dataset)):.4f}, Val Loss Per Team Per Game: {val_loss / (2 * len(val_loader.dataset)):.4f}\")\n",
    "\n",
    "\n",
    "# Testing phase\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        if batch is None:\n",
    "            continue\n",
    "        home_tokens, away_tokens, target = batch\n",
    "        prediction = model(home_tokens, away_tokens)\n",
    "        loss = loss_fn(prediction, target)\n",
    "        test_loss += loss.item()\n",
    "print(f\"Cumulative Test Loss: {test_loss:.4f}\")        \n",
    "print(f\"Test Loss Per Game Per Team: {test_loss/(2*len(test_loader.dataset)):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **III. Hyperparameter Tuning**\n",
    "\n",
    "### Overview\n",
    "This section explores the impact of various hyperparameters on the model's performance. By systematically adjusting key parameters, we aim to optimize the transformer network for better predictions.\n",
    "\n",
    "### Goals\n",
    "1. **Experimentation**:\n",
    "   - Test different values for hyperparameters such as:\n",
    "     - Learning rate.\n",
    "     - Number of epochs.\n",
    "     - Optimizer (e.g., Adam, SGD).\n",
    "     - Batch size.\n",
    "     - Number of transformer layers and attention heads.\n",
    "2. **Performance Evaluation**:\n",
    "   - Assess the impact of each hyperparameter on model accuracy, loss, and F1-score.\n",
    "   - Document observations to identify the most effective configurations.\n",
    "\n",
    "### Implementation Steps\n",
    "1. **Baseline Configuration**:\n",
    "   - Train the model with default or commonly used hyperparameter values.\n",
    "   - Record baseline performance metrics.\n",
    "2. **Iterative Testing**:\n",
    "   - Adjust one hyperparameter at a time while keeping others constant.\n",
    "   - Monitor changes in performance and identify trends.\n",
    "3. **Optimal Configuration**:\n",
    "   - Combine the best-performing hyperparameters into a final configuration for training the model.\n",
    "\n",
    "This section will detail the experiments conducted and the resulting insights into hyperparameter optimization for the transformer network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **IV. Evaluation and Analysis**\n",
    "\n",
    "### Overview\n",
    "In this section, we evaluate the performance of the transformer-based model using two primary loss metrics and other relevant performance indicators. The focus will be on understanding the model's strengths, limitations, and areas for improvement.\n",
    "\n",
    "### Metrics\n",
    "1. **Score Prediction Accuracy**:\n",
    "   - Measure the actual distance between predicted scores and the true game scores (e.g., Mean Squared Error or Mean Absolute Error).\n",
    "   - Assess how well the model captures the scoring trends in games.\n",
    "2. **Winning Outcome Prediction**:\n",
    "   - Evaluate the model's ability to correctly predict the winning team (e.g., Accuracy, F1-score).\n",
    "   - Analyze classification performance using confusion matrices.\n",
    "\n",
    "### Goals\n",
    "1. **Performance Metrics**:\n",
    "   - Quantify how accurately the model predicts game outcomes and scores.\n",
    "   - Identify patterns or biases in the modelâ€™s predictions.\n",
    "2. **Visual Representations**:\n",
    "   - Plot training and validation loss over epochs.\n",
    "   - Generate confusion matrices for winning outcome predictions.\n",
    "   - Visualize score prediction distributions.\n",
    "3. **Strengths and Limitations**:\n",
    "   - Discuss areas where the model performs well and where it struggles.\n",
    "   - Identify real-world scenarios where the model could be applied effectively.\n",
    "4. **Future Improvements**:\n",
    "   - Suggest ways to enhance the model, such as adjusting hyperparameters, adding new features, or increasing dataset size.\n",
    "\n",
    "This section will summarize the model's overall performance, supported by quantitative metrics and visualizations.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
